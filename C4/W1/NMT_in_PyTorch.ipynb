{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4HhgH9fVFjCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bd6746-104c-441e-9dee-76741fd0bbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Google drive setup\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/AttentionModelsCoursera/W1_NMT_Attention/W1/por-eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JllT-5O9lu05",
        "outputId": "3d8900c3-74fb-4dd2-97bc-2308b761c2e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AttentionModelsCoursera/W1_NMT_Attention/W1/por-eng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from utils import (sentences, train_dataset, val_dataset, train_loader, val_loader,\n",
        "                   tokenizer_eng, tokenizer_por, masked_loss, masked_acc, ids_to_text, encode_sample)"
      ],
      "metadata": {
        "id": "7GLdPiE-GQNX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "OxlGeg_dRr5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences, portuguese_sentences = sentences\n",
        "\n",
        "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
        "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKqu8oXERRRm",
        "outputId": "dfa12300-c374-49ae-bea2-c9621abfc330"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English (to translate) sentence:\n",
            "\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
            "\n",
            "Portuguese (translation) sentence:\n",
            "\n",
            "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del portuguese_sentences\n",
        "del english_sentences\n",
        "del sentences"
      ],
      "metadata": {
        "id": "P5_KmRiKmiAt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 10 words of the english vocabulary:\\n\\n{sorted(tokenizer_eng.get_vocab().items(), key=lambda item: item[1])[:10]}\\n\")\n",
        "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{sorted(tokenizer_por.get_vocab().items(), key=lambda item: item[1])[:10]}\")"
      ],
      "metadata": {
        "id": "xzQtQPDzlYOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bf37d7-a341-4f8b-eecb-f0494f18edb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words of the english vocabulary:\n",
            "\n",
            "[('[PAD]', 0), ('[UNK]', 1), ('[EOS]', 2), ('[SOS]', 3), ('.', 4), ('tom', 5), ('i', 6), ('to', 7), ('you', 8), ('the', 9)]\n",
            "\n",
            "First 10 words of the portuguese vocabulary:\n",
            "\n",
            "[('[PAD]', 0), ('[UNK]', 1), ('[EOS]', 2), ('[SOS]', 3), ('.', 4), ('tom', 5), ('que', 6), ('o', 7), ('nao', 8), ('eu', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the vocabulary\n",
        "vocab_size_por = tokenizer_eng.get_vocab_size()\n",
        "vocab_size_eng = tokenizer_eng.get_vocab_size()\n",
        "\n",
        "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
        "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMMIOTCso5GG",
        "outputId": "960224f9-8fd5-4d34-98bb-7565d2b0403f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese vocabulary is made up of 12000 words\n",
            "English vocabulary is made up of 12000 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_id(token):\n",
        "    return tokenizer_por.token_to_id(token)\n",
        "\n",
        "\n",
        "def ids_to_words(id):\n",
        "    return tokenizer_por.id_to_token(id)"
      ],
      "metadata": {
        "id": "-KnZE672xXqm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_id = word_to_id(\"[UNK]\")\n",
        "sos_id = word_to_id(\"[SOS]\")\n",
        "eos_id = word_to_id(\"[EOS]\")\n",
        "baunilha_id = word_to_id(\"baunilha\")\n",
        "\n",
        "print(f\"The id for the [UNK] token is {unk_id}\")\n",
        "print(f\"The id for the [SOS] token is {sos_id}\")\n",
        "print(f\"The id for the [EOS] token is {eos_id}\")\n",
        "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFj-6OSuxYFG",
        "outputId": "68606c95-da88-4c5d-946a-08ae17eba943"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The id for the [UNK] token is 1\n",
            "The id for the [SOS] token is 3\n",
            "The id for the [EOS] token is 2\n",
            "The id for baunilha (vanilla) is 5242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(to_translate, sr_translation), translation = next(iter(train_loader))\n",
        "\n",
        "print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
        "print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
        "print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"Len of Tokenized english sentence:\\n{len(to_translate[0, :].numpy())}\\n\\n\")\n",
        "print(f\"Len of Tokenized portuguese sentence (shifted to the right):\\n{len(sr_translation[0, :].numpy())}\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbLzXYW23nbW",
        "outputId": "95723009-c945-4b36-98db-7851be5bf8ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized english sentence:\n",
            "[   3  173   46   66  282   66   22 2167  793    4    2    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence (shifted to the right):\n",
            "[  3 103 171   6  12 744 378   4   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence:\n",
            "[103 171   6  12 744 378   4   2   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "\n",
            "\n",
            "\n",
            "Len of Tokenized english sentence:\n",
            "42\n",
            "\n",
            "\n",
            "Len of Tokenized portuguese sentence (shifted to the right):\n",
            "49\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(to_translate.size()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaNPlgXyNJxz",
        "outputId": "e067fe30-b14e-470d-80ef-fba2bfbfc1b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "R1vQncuf5WT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 12000\n",
        "UNITS = 256"
      ],
      "metadata": {
        "id": "3yrT4UUA5XWC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(units, units, bidirectional=True, batch_first=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        # Summarizing the bidirectional RNNs to follow the TF version\n",
        "        forward_output = x[:, :, :UNITS]\n",
        "        backward_output = x[:, :, UNITS:]\n",
        "        x = forward_output + backward_output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1Dwpp8_eBF76"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "encoder_output = encoder(to_translate)\n",
        "\n",
        "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
        "print(f'Encoder output has shape: {encoder_output.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coZ1glxoJiRv",
        "outputId": "19e6e093-1d60-4dc2-9db4-a7f1533b4439"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of sentences in english has shape: torch.Size([64, 42])\n",
            "\n",
            "Encoder output has shape: torch.Size([64, 42, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Attention"
      ],
      "metadata": {
        "id": "SfI3vy3cUZ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(units, 1, batch_first=True)\n",
        "        self.layernorm = nn.LayerNorm(units)\n",
        "\n",
        "    def forward(self, context, target):\n",
        "        #print(f\"Cross attn context shape: {context.size()}\")\n",
        "        #print(f\"Cross attn target shape: {target.size()}\")\n",
        "        attn_output = self.mha(query=target,key=context, value=context)\n",
        "        x = target + attn_output[0]\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PkeZr7_vKe_w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "sr_translation_embed = nn.Embedding(VOCAB_SIZE, UNITS, 0)(sr_translation)\n",
        "\n",
        "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
        "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2gsNK36XfgJ",
        "outputId": "0ef58385-ab82-4eda-cce7-cc59612aaa7e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: torch.Size([64, 42, 256])\n",
            "Tensor of translations has shape: torch.Size([64, 49, 256])\n",
            "Tensor of attention scores has shape: torch.Size([64, 49, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "4iuxMJx7cnB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n",
        "        self.pre_attention_rnn = nn.LSTM(units, units, batch_first=True)\n",
        "        self.attention = CrossAttention(units)\n",
        "        self.post_attention_rnn = nn.LSTM(units, units, batch_first=True)\n",
        "        self.output_layer = nn.Linear(units, vocab_size)\n",
        "        self.activation = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, context, target_in, state=None, return_state=False):\n",
        "        x = self.embedding(target_in)\n",
        "        x, (hidden_state, cell_state) = self.pre_attention_rnn(x, state)\n",
        "\n",
        "        #print(f\"context shape: {context.size()}\\n\")\n",
        "        #print(f\"target shape: {x.size()}\\n\")\n",
        "\n",
        "        x = self.attention(context, x)\n",
        "        x, _ = self.post_attention_rnn(x)\n",
        "        x = self.output_layer(x)\n",
        "        logits = self.activation(x)\n",
        "\n",
        "        if return_state:\n",
        "            return logits, [hidden_state, cell_state]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9uaq8tR4cnmj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
        "\n",
        "logits = decoder(encoder_output, sr_translation)\n",
        "\n",
        "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMVYa392xyjW",
        "outputId": "e62da169-facb-4967-bb52-93987b6c368c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: torch.Size([64, 42, 256])\n",
            "Tensor of right-shifted translations has shape: torch.Size([64, 49])\n",
            "Tensor of logits has shape: torch.Size([64, 49, 12000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translator"
      ],
      "metadata": {
        "id": "GPOfZIPYBNNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_sgfznSZWFvo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(vocab_size, units)\n",
        "        self.decoder = Decoder(vocab_size, units)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        context, targets = inputs\n",
        "\n",
        "        encoded_context = self.encoder(context)\n",
        "        logits = self.decoder(encoded_context, targets)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ghu6C60J1jD_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(VOCAB_SIZE, UNITS).to(device)\n",
        "to_translate, sr_translation = to_translate.to(device), sr_translation.to(device)\n",
        "\n",
        "logits = translator((to_translate, sr_translation))\n",
        "\n",
        "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxw4GurPGfsh",
        "outputId": "f527ed49-df4c-418a-a921-3d7e6feb27d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of sentences to translate has shape: torch.Size([64, 42])\n",
            "Tensor of right-shifted translations has shape: torch.Size([64, 49])\n",
            "Tensor of logits has shape: torch.Size([64, 49, 12000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=translator.parameters())\n",
        "criterion = masked_loss\n",
        "acc = masked_acc"
      ],
      "metadata": {
        "id": "Kysqf7SpPg0B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "H9sDgrl3HGvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1\n",
        "STEPS_PER_EPOCH = 10\n",
        "patience = 3\n",
        "min_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Mini batch loss\n",
        "    running_loss = 0.0\n",
        "    # Epoch loss for early stopping\n",
        "    epoch_loss = 0.0\n",
        "    translator.train()\n",
        "\n",
        "    # Using itertools for fixed length iteration over non subscriptable DataLoader\n",
        "    for i, data in enumerate(itertools.islice(train_loader,  STEPS_PER_EPOCH)):\n",
        "        (context, target_in), target_out = data\n",
        "\n",
        "        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = translator((context, target_in))\n",
        "        accuracy = acc(target_out, outputs)\n",
        "        loss = criterion(target_out, outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        #Getting the loss of the epoch\n",
        "        if i+1 == STEPS_PER_EPOCH:\n",
        "            epoch_loss = running_loss\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            print(f\"\\n[epoch: {epoch+1}, mini batch: {i+1}] loss: {running_loss:.4f}, accuracy: {accuracy:.4f}\\n\")\n",
        "            running_loss = 0\n",
        "\n",
        "    # Update the best loss if it's better than the previous one\n",
        "    if epoch_loss < min_loss:\n",
        "        min_loss = epoch_loss\n",
        "        patience = 3\n",
        "\n",
        "    else:\n",
        "        # Losing patience\n",
        "        patience -= 1\n",
        "\n",
        "        if patience == 0:\n",
        "            break"
      ],
      "metadata": {
        "id": "FUYcGoGOHHVS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "amoHG9_Q1Iyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 3\n",
        "min_loss = float('inf')\n",
        "\n",
        "running_loss = 0.0\n",
        "translator.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(itertools.islice(val_loader,  STEPS_PER_EPOCH)):\n",
        "        (context, target_in), target_out = data\n",
        "\n",
        "        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n",
        "\n",
        "        outputs = translator((context, target_in))\n",
        "        loss = criterion(target_out, outputs)\n",
        "        accuracy = acc(target_out, outputs)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            print(f\"\\n[mini batch: {i+1}] validation loss: {running_loss:.4f}, validation accuracy: {accuracy:.4f}\\n\")\n",
        "            running_loss = 0"
      ],
      "metadata": {
        "id": "IMqkbJz31JuS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the model for inference"
      ],
      "metadata": {
        "id": "Nat9JkDJT8Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_token(context, decoder, next_token, done, state, temperature=0.0):\n",
        "    logits, state = decoder(context, next_token, state, return_state=True)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    if temperature == 0.0:\n",
        "        next_token = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    else:\n",
        "        logits = torch.exp(logits)\n",
        "        logits /= temperature\n",
        "        next_token = torch.multinomial(logits, 1)\n",
        "        logits = torch.log(logits)\n",
        "\n",
        "    logits = torch.squeeze(logits)\n",
        "\n",
        "    next_token = torch.squeeze(next_token)\n",
        "\n",
        "    logit = logits[next_token].detach().numpy()\n",
        "\n",
        "    next_token = torch.reshape(next_token, shape=(1,1))\n",
        "\n",
        "    if next_token == eos_id:\n",
        "        done = True\n",
        "\n",
        "    return next_token, logit, state, done"
      ],
      "metadata": {
        "id": "dRVu0cp0T9vu"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentence = \"I love languages\"\n",
        "\n",
        "context = torch.tensor(encode_sample(eng_sentence))\n",
        "context = torch.unsqueeze(context, dim=0)\n",
        "context = encoder(context)\n",
        "\n",
        "next_token = torch.full((1,1), sos_id)\n",
        "\n",
        "state = [torch.rand((1, 1, UNITS)), torch.rand((1, 1, UNITS))]\n",
        "done = False\n",
        "\n",
        "next, logit, state, done = generate_next_token(context, decoder, next_token, done, state, temperature=0.5)\n",
        "print(f\"Next token: {next}\\nLogit: {logit:.4f}\\nDone? {done}\")"
      ],
      "metadata": {
        "id": "E8UgIJZjD2_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc50d58-86b6-4f66-d31b-18051fd7f060"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[0;32m<ipython-input-92-0374d4e60933>\u001b[0m(3)\u001b[0;36mgenerate_next_token\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mgenerate_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "Next token: tensor([[11239]])\n",
            "Logit: -8.7271\n",
            "Done? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate"
      ],
      "metadata": {
        "id": "CDG_-BNy-knN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, text, max_length=50, temperature=0.0):\n",
        "    tokens, logits = [], []\n",
        "\n",
        "    pre_text = text\n",
        "    text = torch.tensor(encode_sample(pre_text))\n",
        "    text = torch.unsqueeze(text, dim=0)\n",
        "\n",
        "    context = encoder(text)\n",
        "\n",
        "    next_token = torch.full((1,1), sos_id)\n",
        "\n",
        "    state = [torch.zeros((1, 1, UNITS)), torch.zeros((1, 1, UNITS))]\n",
        "\n",
        "    done = False\n",
        "\n",
        "    for iteration in range(max_length):\n",
        "        try:\n",
        "            print(f\"next_token: {next_token}\")\n",
        "            next_token, logit, state, done = generate_next_token(\n",
        "                context=context,\n",
        "                decoder=model.decoder,\n",
        "                next_token=next_token,\n",
        "                done=done,\n",
        "                state=state,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            print(f\"logit: {logit}\")\n",
        "\n",
        "        except:\n",
        "            raise Exception(\"Problem generating the next token\")\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        tokens.append(next_token)\n",
        "\n",
        "        logits.append(logit)\n",
        "\n",
        "    tokens = torch.cat(tokens, dim=-1)\n",
        "    tokens = torch.squeeze(tokens, dim=0).detach().numpy()\n",
        "    translation = ids_to_text(tokens, tokenizer_por)\n",
        "\n",
        "    return translation, logits[-1], tokens"
      ],
      "metadata": {
        "id": "ZLqbKnLaxZ6J"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running this cell multiple times should return the same output since temp is 0\n",
        "\n",
        "temp = 0.0\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ],
      "metadata": {
        "id": "dBKxYvlcjX-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5b1254-4c3f-40c1-8c56-0cace5a4282d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_token: tensor([[3]])\n",
            "> \u001b[0;32m<ipython-input-92-0374d4e60933>\u001b[0m(3)\u001b[0;36mgenerate_next_token\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mgenerate_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "logit: -7.592885494232178\n",
            "next_token: tensor([[5]])\n",
            "> \u001b[0;32m<ipython-input-92-0374d4e60933>\u001b[0m(3)\u001b[0;36mgenerate_next_token\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mgenerate_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      2 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "logit: -6.815021514892578\n",
            "Temperature: 0.0\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: tom\n",
            "Translation tokens:[5]\n",
            "Logit: -7.593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foo = torch.tensor([[5]])\n",
        "foo = torch.squeeze(foo, dim=0)\n",
        "foo = foo.detach().numpy()\n",
        "print(foo)\n",
        "translation = ids_to_text(foo, tokenizer_por)\n",
        "translation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Q-9xArgTykFH",
        "outputId": "4a47eb0f-fb7e-436d-a333-8afe085cc2d4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y46oYG6EvDHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56_AjxqLpcpc"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}
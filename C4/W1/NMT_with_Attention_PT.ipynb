{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9478476,"sourceType":"datasetVersion","datasetId":5754974}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cd /kaggle/input/dataset-eng-por","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:27.652008Z","iopub.execute_input":"2024-09-25T19:36:27.652951Z","iopub.status.idle":"2024-09-25T19:36:27.674732Z","shell.execute_reply.started":"2024-09-25T19:36:27.652902Z","shell.execute_reply":"2024-09-25T19:36:27.673825Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-eng-por\n","output_type":"stream"}]},{"cell_type":"code","source":"import pdb\nimport torch\nimport itertools\nimport numpy as np\nimport torch.nn as nn\nfrom collections import Counter\nfrom utils_PT import (sentences, train_dataset, val_dataset, train_loader, val_loader,\n                   tokenizer_eng, tokenizer_por, masked_loss, masked_acc, ids_to_text, encode_sample, pt_lower_and_split_punct)","metadata":{"id":"7GLdPiE-GQNX","execution":{"iopub.status.busy":"2024-09-25T19:36:27.676295Z","iopub.execute_input":"2024-09-25T19:36:27.676603Z","iopub.status.idle":"2024-09-25T19:36:41.695945Z","shell.execute_reply.started":"2024-09-25T19:36:27.676572Z","shell.execute_reply":"2024-09-25T19:36:41.695110Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"_sgfznSZWFvo","execution":{"iopub.status.busy":"2024-09-25T19:36:41.697038Z","iopub.execute_input":"2024-09-25T19:36:41.697463Z","iopub.status.idle":"2024-09-25T19:36:41.734077Z","shell.execute_reply.started":"2024-09-25T19:36:41.697430Z","shell.execute_reply":"2024-09-25T19:36:41.733116Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{"id":"OxlGeg_dRr5F"}},{"cell_type":"code","source":"english_sentences, portuguese_sentences = sentences\n\nprint(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\nprint(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKqu8oXERRRm","outputId":"dfa12300-c374-49ae-bea2-c9621abfc330","execution":{"iopub.status.busy":"2024-09-25T19:36:41.736794Z","iopub.execute_input":"2024-09-25T19:36:41.737600Z","iopub.status.idle":"2024-09-25T19:36:41.744651Z","shell.execute_reply.started":"2024-09-25T19:36:41.737554Z","shell.execute_reply":"2024-09-25T19:36:41.743791Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"English (to translate) sentence:\n\nNo matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n\nPortuguese (translation) sentence:\n\nNão importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n","output_type":"stream"}]},{"cell_type":"code","source":"del portuguese_sentences\ndel english_sentences\ndel sentences","metadata":{"id":"P5_KmRiKmiAt","execution":{"iopub.status.busy":"2024-09-25T19:36:41.745776Z","iopub.execute_input":"2024-09-25T19:36:41.746158Z","iopub.status.idle":"2024-09-25T19:36:41.754509Z","shell.execute_reply.started":"2024-09-25T19:36:41.746125Z","shell.execute_reply":"2024-09-25T19:36:41.753716Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(f\"First 10 words of the english vocabulary:\\n\\n{sorted(tokenizer_eng.get_vocab().items(), key=lambda item: item[1])[:10]}\\n\")\nprint(f\"First 10 words of the portuguese vocabulary:\\n\\n{sorted(tokenizer_por.get_vocab().items(), key=lambda item: item[1])[:10]}\")","metadata":{"id":"xzQtQPDzlYOH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5bf37d7-a341-4f8b-eecb-f0494f18edb7","execution":{"iopub.status.busy":"2024-09-25T19:36:41.755555Z","iopub.execute_input":"2024-09-25T19:36:41.755873Z","iopub.status.idle":"2024-09-25T19:36:41.815885Z","shell.execute_reply.started":"2024-09-25T19:36:41.755807Z","shell.execute_reply":"2024-09-25T19:36:41.814828Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"First 10 words of the english vocabulary:\n\n[('[PAD]', 0), ('[UNK]', 1), ('[EOS]', 2), ('[SOS]', 3), ('.', 4), ('tom', 5), ('i', 6), ('to', 7), ('you', 8), ('the', 9)]\n\nFirst 10 words of the portuguese vocabulary:\n\n[('[PAD]', 0), ('[UNK]', 1), ('[EOS]', 2), ('[SOS]', 3), ('.', 4), ('tom', 5), ('que', 6), ('o', 7), ('nao', 8), ('eu', 9)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Size of the vocabulary\nvocab_size_por = tokenizer_eng.get_vocab_size()\nvocab_size_eng = tokenizer_eng.get_vocab_size()\n\nprint(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\nprint(f\"English vocabulary is made up of {vocab_size_eng} words\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMMIOTCso5GG","outputId":"960224f9-8fd5-4d34-98bb-7565d2b0403f","execution":{"iopub.status.busy":"2024-09-25T19:36:41.817141Z","iopub.execute_input":"2024-09-25T19:36:41.817811Z","iopub.status.idle":"2024-09-25T19:36:41.825818Z","shell.execute_reply.started":"2024-09-25T19:36:41.817768Z","shell.execute_reply":"2024-09-25T19:36:41.824852Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Portuguese vocabulary is made up of 12000 words\nEnglish vocabulary is made up of 12000 words\n","output_type":"stream"}]},{"cell_type":"code","source":"def word_to_id(token):\n    return tokenizer_por.token_to_id(token)\n\n\ndef ids_to_words(id):\n    return tokenizer_por.id_to_token(id)","metadata":{"id":"-KnZE672xXqm","execution":{"iopub.status.busy":"2024-09-25T19:36:41.827008Z","iopub.execute_input":"2024-09-25T19:36:41.827340Z","iopub.status.idle":"2024-09-25T19:36:41.837860Z","shell.execute_reply.started":"2024-09-25T19:36:41.827309Z","shell.execute_reply":"2024-09-25T19:36:41.837025Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"unk_id = word_to_id(\"[UNK]\")\nsos_id = word_to_id(\"[SOS]\")\neos_id = word_to_id(\"[EOS]\")\nbaunilha_id = word_to_id(\"baunilha\")\n\nprint(f\"The id for the [UNK] token is {unk_id}\")\nprint(f\"The id for the [SOS] token is {sos_id}\")\nprint(f\"The id for the [EOS] token is {eos_id}\")\nprint(f\"The id for baunilha (vanilla) is {baunilha_id}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFj-6OSuxYFG","outputId":"68606c95-da88-4c5d-946a-08ae17eba943","execution":{"iopub.status.busy":"2024-09-25T19:36:41.838927Z","iopub.execute_input":"2024-09-25T19:36:41.839223Z","iopub.status.idle":"2024-09-25T19:36:41.849316Z","shell.execute_reply.started":"2024-09-25T19:36:41.839170Z","shell.execute_reply":"2024-09-25T19:36:41.848456Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The id for the [UNK] token is 1\nThe id for the [SOS] token is 3\nThe id for the [EOS] token is 2\nThe id for baunilha (vanilla) is 5242\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TODO: SO, there's 2 options why the inference is wrong:\n1. Because the model learns the wrong pattern (either the preprocessing is incorrect or other) and the inference is correct.\n2. Or because the model learns the right pattern and the inference is implemented incorrectly.\n\n\nChecking #1 first:\nin the cell bellow preprocess exactly the same data sample for both TF and PT versions to check if our version outputs the same ","metadata":{}},{"cell_type":"code","source":"(to_translate, sr_translation), translation = next(iter(train_loader))\n\nprint(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\nprint(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\nprint(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HbLzXYW23nbW","outputId":"95723009-c945-4b36-98db-7851be5bf8ed","execution":{"iopub.status.busy":"2024-09-25T19:36:41.853553Z","iopub.execute_input":"2024-09-25T19:36:41.853818Z","iopub.status.idle":"2024-09-25T19:36:41.936106Z","shell.execute_reply.started":"2024-09-25T19:36:41.853789Z","shell.execute_reply":"2024-09-25T19:36:41.935139Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Tokenized english sentence:\n[   3  173   46   66  282   66   22 2167  793    4    2    0    0    0\n    0    0    0    0    0]\n\n\nTokenized portuguese sentence (shifted to the right):\n[  3 103 171   6  12 744 378   4   0   0   0   0   0   0   0   0   0   0\n   0]\n\n\nTokenized portuguese sentence:\n[103 171   6  12 744 378   4   2   0   0   0   0   0   0   0   0   0   0\n   0]\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Encoder","metadata":{"id":"R1vQncuf5WT1"}},{"cell_type":"code","source":"VOCAB_SIZE = 12000\nUNITS = 256","metadata":{"id":"3yrT4UUA5XWC","execution":{"iopub.status.busy":"2024-09-25T19:36:41.937242Z","iopub.execute_input":"2024-09-25T19:36:41.937520Z","iopub.status.idle":"2024-09-25T19:36:41.941577Z","shell.execute_reply.started":"2024-09-25T19:36:41.937490Z","shell.execute_reply":"2024-09-25T19:36:41.940595Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n        self.rnn = nn.LSTM(units, units, bidirectional=True, batch_first=True)\n\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        # Summarizing the bidirectional RNNs to follow the TF version\n        forward_output = x[:, :, :UNITS]\n        backward_output = x[:, :, UNITS:]\n        x = forward_output + backward_output\n\n        return x","metadata":{"id":"1Dwpp8_eBF76","execution":{"iopub.status.busy":"2024-09-25T19:36:41.942784Z","iopub.execute_input":"2024-09-25T19:36:41.943101Z","iopub.status.idle":"2024-09-25T19:36:41.951127Z","shell.execute_reply.started":"2024-09-25T19:36:41.943068Z","shell.execute_reply":"2024-09-25T19:36:41.950151Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(VOCAB_SIZE, UNITS)\n\nencoder_output = encoder(to_translate)\n\nprint(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\nprint(f'Encoder output has shape: {encoder_output.shape}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coZ1glxoJiRv","outputId":"19e6e093-1d60-4dc2-9db4-a7f1533b4439","execution":{"iopub.status.busy":"2024-09-25T19:36:41.952364Z","iopub.execute_input":"2024-09-25T19:36:41.952731Z","iopub.status.idle":"2024-09-25T19:36:42.166387Z","shell.execute_reply.started":"2024-09-25T19:36:41.952696Z","shell.execute_reply":"2024-09-25T19:36:42.165403Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Tensor of sentences in english has shape: torch.Size([64, 19])\n\nEncoder output has shape: torch.Size([64, 19, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(to_translate[0].shape)\nprint(to_translate[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.167733Z","iopub.execute_input":"2024-09-25T19:36:42.168045Z","iopub.status.idle":"2024-09-25T19:36:42.175985Z","shell.execute_reply.started":"2024-09-25T19:36:42.168012Z","shell.execute_reply":"2024-09-25T19:36:42.175038Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([19])\ntensor([   3,  173,   46,   66,  282,   66,   22, 2167,  793,    4,    2,    0,\n           0,    0,    0,    0,    0,    0,    0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ids_to_text([to_translate[0].tolist()], tokenizer_eng))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.177242Z","iopub.execute_input":"2024-09-25T19:36:42.177589Z","iopub.status.idle":"2024-09-25T19:36:42.183525Z","shell.execute_reply.started":"2024-09-25T19:36:42.177531Z","shell.execute_reply":"2024-09-25T19:36:42.182682Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['[SOS] lets go as soon as it stops raining . [EOS]']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(sr_translation[0].shape)\nprint(sr_translation[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.184620Z","iopub.execute_input":"2024-09-25T19:36:42.184938Z","iopub.status.idle":"2024-09-25T19:36:42.193454Z","shell.execute_reply.started":"2024-09-25T19:36:42.184901Z","shell.execute_reply":"2024-09-25T19:36:42.192498Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"torch.Size([19])\ntensor([  3, 103, 171,   6,  12, 744, 378,   4,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ids_to_text([sr_translation[0].tolist()], tokenizer_por))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.194485Z","iopub.execute_input":"2024-09-25T19:36:42.194814Z","iopub.status.idle":"2024-09-25T19:36:42.205421Z","shell.execute_reply.started":"2024-09-25T19:36:42.194784Z","shell.execute_reply":"2024-09-25T19:36:42.204580Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['[SOS] vamos assim que a chuva parar .']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(translation[0].shape)\nprint(translation[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.206568Z","iopub.execute_input":"2024-09-25T19:36:42.206856Z","iopub.status.idle":"2024-09-25T19:36:42.214925Z","shell.execute_reply.started":"2024-09-25T19:36:42.206811Z","shell.execute_reply":"2024-09-25T19:36:42.213996Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"torch.Size([19])\ntensor([103, 171,   6,  12, 744, 378,   4,   2,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ids_to_text([translation[0].tolist()], tokenizer_por))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:42.216186Z","iopub.execute_input":"2024-09-25T19:36:42.216529Z","iopub.status.idle":"2024-09-25T19:36:42.223419Z","shell.execute_reply.started":"2024-09-25T19:36:42.216498Z","shell.execute_reply":"2024-09-25T19:36:42.222545Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"['vamos assim que a chuva parar . [EOS]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cross Attention","metadata":{"id":"SfI3vy3cUZ3Q"}},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, units):\n        super().__init__()\n\n        self.mha = nn.MultiheadAttention(units, 1, batch_first=True)\n        self.layernorm = nn.LayerNorm(units)\n\n    def forward(self, context, target):\n        attn_output = self.mha(query=target,key=context, value=context)\n        x = target + attn_output[0] # [0] because we only need the attention output and no weights\n        x = self.layernorm(x) \n\n        return x","metadata":{"id":"PkeZr7_vKe_w","execution":{"iopub.status.busy":"2024-09-25T19:36:42.224516Z","iopub.execute_input":"2024-09-25T19:36:42.224802Z","iopub.status.idle":"2024-09-25T19:36:42.232619Z","shell.execute_reply.started":"2024-09-25T19:36:42.224772Z","shell.execute_reply":"2024-09-25T19:36:42.231773Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"attention_layer = CrossAttention(UNITS)\n\nsr_translation_embed = nn.Embedding(VOCAB_SIZE, UNITS, 0)(sr_translation)\n\nattention_result = attention_layer(encoder_output, sr_translation_embed)\n\nprint(f'Tensor of contexts has shape: {encoder_output.shape}')\nprint(f'Tensor of translations has shape: {sr_translation_embed.shape}')\nprint(f'Tensor of attention scores has shape: {attention_result.shape}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2gsNK36XfgJ","outputId":"0ef58385-ab82-4eda-cce7-cc59612aaa7e","execution":{"iopub.status.busy":"2024-09-25T19:36:42.233685Z","iopub.execute_input":"2024-09-25T19:36:42.233976Z","iopub.status.idle":"2024-09-25T19:36:42.347281Z","shell.execute_reply.started":"2024-09-25T19:36:42.233946Z","shell.execute_reply":"2024-09-25T19:36:42.346338Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Tensor of contexts has shape: torch.Size([64, 19, 256])\nTensor of translations has shape: torch.Size([64, 19, 256])\nTensor of attention scores has shape: torch.Size([64, 19, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Decoder","metadata":{"id":"4iuxMJx7cnB4"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n        self.pre_attention_rnn = nn.LSTM(units, units, batch_first=True)\n        self.attention = CrossAttention(units)\n        self.post_attention_rnn = nn.LSTM(units, units, batch_first=True)\n        self.output_layer = nn.Linear(units, vocab_size)\n        self.activation = nn.LogSoftmax(dim=-1)\n\n    def forward(self, context, target_in, state=None, return_state=False):\n        x = self.embedding(target_in)\n        x, (hidden_state, cell_state) = self.pre_attention_rnn(x, state)\n        x = self.attention(context, x)\n        x, _ = self.post_attention_rnn(x)\n        x = self.output_layer(x)\n        logits = self.activation(x)\n\n        if return_state:\n            return logits, [hidden_state, cell_state]\n\n        return logits","metadata":{"id":"9uaq8tR4cnmj","execution":{"iopub.status.busy":"2024-09-25T19:36:42.348562Z","iopub.execute_input":"2024-09-25T19:36:42.348955Z","iopub.status.idle":"2024-09-25T19:36:42.358574Z","shell.execute_reply.started":"2024-09-25T19:36:42.348912Z","shell.execute_reply":"2024-09-25T19:36:42.357620Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(VOCAB_SIZE, UNITS)\n\nlogits = decoder(encoder_output, sr_translation)\n\nprint(f'Tensor of contexts has shape: {encoder_output.shape}')\nprint(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\nprint(f'Tensor of logits has shape: {logits.shape}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMVYa392xyjW","outputId":"e62da169-facb-4967-bb52-93987b6c368c","execution":{"iopub.status.busy":"2024-09-25T19:36:42.360054Z","iopub.execute_input":"2024-09-25T19:36:42.360388Z","iopub.status.idle":"2024-09-25T19:36:42.547847Z","shell.execute_reply.started":"2024-09-25T19:36:42.360355Z","shell.execute_reply":"2024-09-25T19:36:42.546882Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Tensor of contexts has shape: torch.Size([64, 19, 256])\nTensor of right-shifted translations has shape: torch.Size([64, 19])\nTensor of logits has shape: torch.Size([64, 19, 12000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Translator","metadata":{"id":"GPOfZIPYBNNu"}},{"cell_type":"code","source":"class Translator(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.encoder = Encoder(vocab_size, units)\n        self.decoder = Decoder(vocab_size, units)\n\n    def forward(self, inputs):\n        context, targets = inputs\n\n        encoded_context = self.encoder(context)\n        logits = self.decoder(encoded_context, targets)\n\n        return logits","metadata":{"id":"ghu6C60J1jD_","execution":{"iopub.status.busy":"2024-09-25T19:36:42.549001Z","iopub.execute_input":"2024-09-25T19:36:42.549327Z","iopub.status.idle":"2024-09-25T19:36:42.555348Z","shell.execute_reply.started":"2024-09-25T19:36:42.549293Z","shell.execute_reply":"2024-09-25T19:36:42.554343Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"translator = Translator(VOCAB_SIZE, UNITS).to(device)\n\n# Loading the model\n#translator.load_state_dict(torch.load('/kaggle/working/model_weights.pth', map_location=torch.device(device), weights_only=True))\n\nlogits = translator((to_translate.to(device), sr_translation.to(device)))\n\nprint(f'Tensor of sentences to translate has shape: {to_translate.shape}')\nprint(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\nprint(f'Tensor of logits has shape: {logits.shape}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hxw4GurPGfsh","outputId":"f527ed49-df4c-418a-a921-3d7e6feb27d2","execution":{"iopub.status.busy":"2024-09-25T19:36:42.556495Z","iopub.execute_input":"2024-09-25T19:36:42.556791Z","iopub.status.idle":"2024-09-25T19:36:43.238351Z","shell.execute_reply.started":"2024-09-25T19:36:42.556760Z","shell.execute_reply":"2024-09-25T19:36:43.237403Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Tensor of sentences to translate has shape: torch.Size([64, 19])\nTensor of right-shifted translations has shape: torch.Size([64, 19])\nTensor of logits has shape: torch.Size([64, 19, 12000])\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=translator.parameters())\ncriterion = masked_loss\nacc = masked_acc","metadata":{"id":"Kysqf7SpPg0B","execution":{"iopub.status.busy":"2024-09-25T19:36:43.239693Z","iopub.execute_input":"2024-09-25T19:36:43.240374Z","iopub.status.idle":"2024-09-25T19:36:44.051178Z","shell.execute_reply.started":"2024-09-25T19:36:43.240328Z","shell.execute_reply":"2024-09-25T19:36:44.050240Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"H9sDgrl3HGvD"}},{"cell_type":"code","source":"\"\"\"NUM_EPOCHS = 20\nSTEPS_PER_EPOCH = 500\npatience = 3\nmin_loss = float('inf')\n\nfor epoch in range(NUM_EPOCHS):\n    # Mini batch loss\n    running_loss = 0.0\n    # Epoch loss for early stopping\n    epoch_loss = 0.0\n    translator.train()\n\n    # Using itertools for fixed length iteration over non subscriptable DataLoader\n    for i, data in enumerate(itertools.islice(train_loader,  STEPS_PER_EPOCH)):\n        (context, target_in), target_out = data\n\n        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n\n        optimizer.zero_grad()\n        outputs = translator((context, target_in))\n        accuracy = acc(target_out, outputs)\n        loss = criterion(target_out, outputs)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        #Getting the loss of the epoch\n        if i+1 == STEPS_PER_EPOCH:\n            epoch_loss = running_loss\n\n        if i % 100 == 99:\n            print(f\"\\n[epoch: {epoch+1}, mini batch: {i+1}] loss: {running_loss:.4f}, accuracy: {accuracy:.4f}\\n\")\n            running_loss = 0\n\n    # Update the best loss if it's better than the previous one\n    if epoch_loss < min_loss:\n        min_loss = epoch_loss\n        patience = 3\n\n    else:\n        # Losing patience\n        patience -= 1\n\n        if patience == 0:\n            print(\"Early stopping was triggered\")\"\"\"","metadata":{"id":"FUYcGoGOHHVS","_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-25T19:36:44.052269Z","iopub.execute_input":"2024-09-25T19:36:44.052631Z","iopub.status.idle":"2024-09-25T19:36:44.061079Z","shell.execute_reply.started":"2024-09-25T19:36:44.052600Z","shell.execute_reply":"2024-09-25T19:36:44.060236Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'NUM_EPOCHS = 20\\nSTEPS_PER_EPOCH = 500\\npatience = 3\\nmin_loss = float(\\'inf\\')\\n\\nfor epoch in range(NUM_EPOCHS):\\n    # Mini batch loss\\n    running_loss = 0.0\\n    # Epoch loss for early stopping\\n    epoch_loss = 0.0\\n    translator.train()\\n\\n    # Using itertools for fixed length iteration over non subscriptable DataLoader\\n    for i, data in enumerate(itertools.islice(train_loader,  STEPS_PER_EPOCH)):\\n        (context, target_in), target_out = data\\n\\n        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\\n\\n        optimizer.zero_grad()\\n        outputs = translator((context, target_in))\\n        accuracy = acc(target_out, outputs)\\n        loss = criterion(target_out, outputs)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n\\n        #Getting the loss of the epoch\\n        if i+1 == STEPS_PER_EPOCH:\\n            epoch_loss = running_loss\\n\\n        if i % 100 == 99:\\n            print(f\"\\n[epoch: {epoch+1}, mini batch: {i+1}] loss: {running_loss:.4f}, accuracy: {accuracy:.4f}\\n\")\\n            running_loss = 0\\n\\n    # Update the best loss if it\\'s better than the previous one\\n    if epoch_loss < min_loss:\\n        min_loss = epoch_loss\\n        patience = 3\\n\\n    else:\\n        # Losing patience\\n        patience -= 1\\n\\n        if patience == 0:\\n            print(\"Early stopping was triggered\")'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Validation","metadata":{"id":"amoHG9_Q1Iyu"}},{"cell_type":"code","source":"STEPS_PER_EPOCH = 500\npatience = 3\nmin_loss = float('inf')\n\nrunning_loss = 0.0\ntranslator.eval()\n\nwith torch.no_grad():\n    for i, data in enumerate(itertools.islice(val_loader,  STEPS_PER_EPOCH)):\n        (context, target_in), target_out = data\n\n        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n\n        outputs = translator((context, target_in))\n        loss = criterion(target_out, outputs)\n        accuracy = acc(target_out, outputs)\n\n        running_loss += loss.item()\n\n        if i % 100 == 99:\n            print(f\"\\n[mini batch: {i+1}] validation loss: {running_loss:.4f}, validation accuracy: {accuracy:.4f}\\n\")\n            running_loss = 0","metadata":{"id":"IMqkbJz31JuS","execution":{"iopub.status.busy":"2024-09-25T19:36:44.067418Z","iopub.execute_input":"2024-09-25T19:36:44.067703Z","iopub.status.idle":"2024-09-25T19:36:48.979640Z","shell.execute_reply.started":"2024-09-25T19:36:44.067674Z","shell.execute_reply":"2024-09-25T19:36:48.978700Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\n[mini batch: 100] validation loss: 939.8885, validation accuracy: 0.0000\n\n\n[mini batch: 200] validation loss: 939.9164, validation accuracy: 0.0000\n\n\n[mini batch: 300] validation loss: 939.9967, validation accuracy: 0.0000\n\n\n[mini batch: 400] validation loss: 939.9012, validation accuracy: 0.0000\n\n\n[mini batch: 500] validation loss: 939.9417, validation accuracy: 0.0000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using the model for inference","metadata":{"id":"Nat9JkDJT8Tg"}},{"cell_type":"code","source":"def generate_next_token(context, decoder, next_token, state, done, temperature=0.0):\n    logits, state = decoder(context, next_token, state, return_state=True)\n    logits = logits[:, -1, :]\n\n    if temperature == 0.0:\n        next_token = torch.argmax(logits, dim=-1)\n\n    else:\n        logits = torch.exp(logits)\n        logits /= temperature\n        next_token = torch.multinomial(logits, 1)\n        logits = torch.log(logits)\n\n    logits = torch.squeeze(logits)\n\n    next_token = torch.squeeze(next_token)\n\n    logit = logits[next_token].detach().numpy()\n\n    next_token = torch.reshape(next_token, shape=(1,1))\n\n    if next_token == eos_id:\n        done = True\n\n    return next_token, logit, state, done","metadata":{"id":"dRVu0cp0T9vu","execution":{"iopub.status.busy":"2024-09-25T19:36:48.980995Z","iopub.execute_input":"2024-09-25T19:36:48.981346Z","iopub.status.idle":"2024-09-25T19:36:48.988737Z","shell.execute_reply.started":"2024-09-25T19:36:48.981313Z","shell.execute_reply":"2024-09-25T19:36:48.987788Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"eng_sentence = \"I love languages\"\n\ncontext = torch.tensor(encode_sample(eng_sentence))\ncontext = torch.unsqueeze(context, dim=0)\ncontext = encoder(context)\n\nnext_token = torch.full((1,1), sos_id)\n\nstate = [torch.rand((1, 1, UNITS)), torch.rand((1, 1, UNITS))]\ndone = False\n\nnext_token, logit, state, done = generate_next_token(context, decoder, next_token, state, done, temperature=0.5)\nprint(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")\nnext_token = next_token.tolist()\nprint(ids_to_text(next_token, tokenizer_por))","metadata":{"id":"E8UgIJZjD2_F","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0dc50d58-86b6-4f66-d31b-18051fd7f060","execution":{"iopub.status.busy":"2024-09-25T19:36:48.989992Z","iopub.execute_input":"2024-09-25T19:36:48.990634Z","iopub.status.idle":"2024-09-25T19:36:49.057565Z","shell.execute_reply.started":"2024-09-25T19:36:48.990590Z","shell.execute_reply":"2024-09-25T19:36:49.056620Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Next token: tensor([[616]])\nLogit: -8.6960\nDone? False\n['gostou']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Translate","metadata":{"id":"CDG_-BNy-knN"}},{"cell_type":"code","source":"def translate(model, text, max_length=50, temperature=0.0):\n    \n    tokens, logits = [], []\n\n    pre_text = text\n    text = torch.tensor(encode_sample(pre_text))\n    text = torch.unsqueeze(text, dim=0)\n\n    context = encoder(text)\n\n    next_token = torch.full((1,1), sos_id)\n\n    # Try uniform instead of zeros here\n    state = [torch.zeros((1, 1, UNITS)), torch.zeros((1, 1, UNITS))]\n\n    done = False\n    for iteration in range(max_length):\n        try:\n            next_token, logit, state, done = generate_next_token(\n                context=context,\n                decoder=model.decoder,\n                next_token=next_token,\n                state=state,\n                done=done,\n                temperature=temperature\n            )\n        except:\n            raise Exception(\"Problem generating the next token\")\n\n        if done:\n            break\n            \n        tokens.append(next_token)\n        \n        logits.append(logit)\n\n    tokens = torch.cat(tokens, dim=-1).tolist()\n    \n    translation = ids_to_text(tokens, tokenizer_por)\n\n    return translation, logits[-1], tokens","metadata":{"id":"ZLqbKnLaxZ6J","execution":{"iopub.status.busy":"2024-09-25T19:36:49.059040Z","iopub.execute_input":"2024-09-25T19:36:49.059458Z","iopub.status.idle":"2024-09-25T19:36:49.068852Z","shell.execute_reply.started":"2024-09-25T19:36:49.059416Z","shell.execute_reply":"2024-09-25T19:36:49.067874Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Running this cell multiple times should return the same output since temp is 0\n\ntemp = 0.0\noriginal_sentence = \"I am Christian\"\n\ntranslation, logit, tokens = translate(translator.to(\"cpu\"), original_sentence, temperature=temp)\n\nprint(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")","metadata":{"id":"dBKxYvlcjX-F","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d5b1254-4c3f-40c1-8c56-0cace5a4282d","execution":{"iopub.status.busy":"2024-09-25T19:36:49.070382Z","iopub.execute_input":"2024-09-25T19:36:49.070774Z","iopub.status.idle":"2024-09-25T19:36:49.248534Z","shell.execute_reply.started":"2024-09-25T19:36:49.070722Z","shell.execute_reply":"2024-09-25T19:36:49.247584Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Temperature: 0.0\n\nOriginal sentence: I am Christian\nTranslation: ['visitara traduza escolar calmos dilema medieval tamanho esteve esteve comestivel controlada bicicletas curto nativo elefantes encontrar teus carvalho universitarios voltarmos reviver reviver guaxinim ileso dei controlada controlada bicicletas curto atica consciencia comunicam atica ensinasse bocejar dormido excecoes equipamentos inclinou inclinou gelada aceitaria fibras atualizar charutos vemos faleceu numero medieval gire']\nTranslation tokens:[[6538, 4160, 3881, 5626, 8514, 8889, 1523, 586, 586, 5657, 8404, 3836, 2264, 1698, 2570, 268, 1659, 7347, 11146, 9333, 10933, 10933, 6875, 10329, 854, 8404, 8404, 3836, 2264, 11591, 3439, 7392, 11591, 10051, 5615, 4692, 5021, 8584, 5819, 5819, 3248, 3546, 10216, 8221, 11875, 3190, 2117, 590, 8889, 10284]]\nLogit: -9.114\n","output_type":"stream"}]},{"cell_type":"code","source":"# Running this cell multiple times should return different outputs since temp is not 0\n# You can try different temperatures\n\ntemp = 0.7\noriginal_sentence = \"I love languages\"\n\ntranslation, logit, tokens = translate(translator.to(\"cpu\"), original_sentence, temperature=temp)\n\nprint(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:36:49.249792Z","iopub.execute_input":"2024-09-25T19:36:49.250227Z","iopub.status.idle":"2024-09-25T19:36:49.392448Z","shell.execute_reply.started":"2024-09-25T19:36:49.250163Z","shell.execute_reply":"2024-09-25T19:36:49.391472Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Temperature: 0.7\n\nOriginal sentence: I love languages\nTranslation: ['caminhonete fundamento trilha cale topo dirigisse cedo juntar ananas convencela retorica tirar largos russa recurso presidenciais voo cacador baleia lago perdoado antartica baixo sugerindo cantando gostarias matar bosque relaxar cuidei vencedores penhasco trabalhamos psicologia mostrarlhes escrevame impotente desanimar jurei importo exagera espirrar barro grecia atarefada luto fuso ganhou tristeza caido']\nTranslation tokens:[[6091, 10254, 8032, 7334, 2998, 5325, 232, 2416, 11438, 8405, 5961, 703, 7704, 7939, 5946, 10791, 935, 7328, 3834, 1176, 7008, 11450, 1054, 6486, 1611, 4275, 1008, 6081, 2533, 4973, 11173, 6391, 2386, 6423, 10549, 6223, 7662, 8480, 5413, 1354, 6244, 10121, 11669, 4731, 11581, 7724, 10257, 890, 7165, 6640]]\nLogit: -9.080\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Minimum Bayes-Risk Decoding","metadata":{}},{"cell_type":"code","source":"def generate_samples(model, text, n_samples=4, temperature=0.6):\n    samples, log_probs = [], []\n    \n    for _ in range(n_samples):\n        _, log_prob, sample = translate(model, text, temperature=temperature)\n        \n        samples.append(sample)\n        \n        log_probs.append(log_prob)\n        \n    return samples, log_probs","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:29:33.958547Z","iopub.execute_input":"2024-09-25T20:29:33.958930Z","iopub.status.idle":"2024-09-25T20:29:33.964732Z","shell.execute_reply.started":"2024-09-25T20:29:33.958896Z","shell.execute_reply":"2024-09-25T20:29:33.963745Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"samples, log_probs = generate_samples(translator, 'I love languages')\n\nfor s, l in zip(samples, log_probs):\n    print(f\"Translated tensor: {s} has logit: {l:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:29:34.508173Z","iopub.execute_input":"2024-09-25T20:29:34.508902Z","iopub.status.idle":"2024-09-25T20:29:35.075944Z","shell.execute_reply.started":"2024-09-25T20:29:34.508862Z","shell.execute_reply":"2024-09-25T20:29:35.074917Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Translated tensor: [[1011, 8825, 786, 455, 8227, 11800, 9455, 835, 6202, 9944, 4677, 1601, 8496, 7975, 105, 6434, 5956, 3894, 1334, 11818, 8215, 3936, 2118, 9073, 5334, 3641, 4459, 10467, 1674, 4272, 7239, 10243, 11423, 11122, 7035, 8976, 2605, 5549, 771, 5412, 4563, 11599, 9811, 11277, 3826, 519, 986, 2122, 11435, 9372]] has logit: -8.967\nTranslated tensor: [[11050, 3894, 9757, 3674, 7566, 5674, 8020, 111, 6804, 10729, 9593, 5570, 2119, 96, 5485, 5263, 9972, 316, 10605, 11926, 5123, 10295, 4584, 1845, 8900, 11130, 6444, 5995, 5007, 8439, 4678, 776, 11417, 4219, 10144, 5719, 4496, 7545, 3568, 3801, 5497, 8081, 9399, 3212, 1540, 4325, 419, 8290, 3624, 5575]] has logit: -8.985\nTranslated tensor: [[9500, 8851, 8305, 10237, 45, 8254, 6269, 8251, 10604, 9082, 11238, 9226, 9588, 7147, 11821, 1256, 1449, 1397, 1518, 3643, 8749, 5885, 6073, 2989, 10600, 1691, 5838, 6212, 10378, 6688, 1571, 6725, 295, 7776, 8293, 5828, 1332, 1714, 11037, 11563, 370, 4184, 1377, 5320, 6696, 5805, 9105, 11626, 4377, 2564]] has logit: -9.040\nTranslated tensor: [[5875, 3464, 8293, 1637, 1777, 5782, 11343, 3785, 2686, 7837, 4393, 11718, 1738, 11624, 2218, 2780, 4842, 2899, 11366, 11282, 9586, 5826, 1746, 3212, 3468, 10701, 3489, 915, 7428, 6571, 3098, 3044, 10735, 2717, 5865, 316, 2703, 261, 2670, 11016, 362, 8446, 4961, 9138, 3291, 7688, 6491, 8768, 412, 4979]] has logit: -8.885\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Comparing overlaps","metadata":{}},{"cell_type":"code","source":"def jaccard_similarity(candidate, reference):\n    \n    if (isinstance(candidate, list) and all(isinstance(i, list) for i in candidate)) and \\\n       (isinstance(reference, list) and all(isinstance(i, list) for i in reference)):\n        candidate_set = set(candidate[0])\n        reference_set = set(reference[0])\n\n    else:\n        candidate_set = set(candidate)\n        reference_set = set(reference)    \n    \n    common_tokens = candidate_set.intersection(reference_set)\n    \n    all_tokens = candidate_set.union(reference_set)\n    \n    overlap = len(common_tokens) / len(all_tokens)\n    \n    return overlap","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:18.650675Z","iopub.execute_input":"2024-09-25T20:43:18.651090Z","iopub.status.idle":"2024-09-25T20:43:18.658021Z","shell.execute_reply.started":"2024-09-25T20:43:18.651054Z","shell.execute_reply":"2024-09-25T20:43:18.656994Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"l1 = [1,2,3]\nl2 = [1,2,3,4]\n\njs = jaccard_similarity(l1, l2)\n\nprint(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:18.887395Z","iopub.execute_input":"2024-09-25T20:43:18.887798Z","iopub.status.idle":"2024-09-25T20:43:18.893647Z","shell.execute_reply.started":"2024-09-25T20:43:18.887764Z","shell.execute_reply":"2024-09-25T20:43:18.892726Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Rouge1 similarity","metadata":{}},{"cell_type":"code","source":"def rouge1_similarity(candidate, reference):\n    candidate_word_counts = Counter(candidate)\n    reference_word_counts = Counter(reference)    \n    \n    overlap = 0\n    \n    for token in candidate_word_counts.keys():\n        token_count_candidate = candidate_word_counts[token]\n        token_count_reference = reference_word_counts[token]        \n        \n        overlap += min(token_count_candidate, token_count_reference)\n        \n    precision = overlap / len(candidate)\n    \n    recall = overlap / len(reference)\n    \n    if precision + recall != 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score\n    \n    return 0","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:19.977174Z","iopub.execute_input":"2024-09-25T20:43:19.977625Z","iopub.status.idle":"2024-09-25T20:43:19.984588Z","shell.execute_reply.started":"2024-09-25T20:43:19.977588Z","shell.execute_reply":"2024-09-25T20:43:19.983591Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"l1 = [0, 1]\nl2 = [5, 5, 7, 0, 232]\n\nr1s = rouge1_similarity(l1, l2)\n\nprint(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:20.616838Z","iopub.execute_input":"2024-09-25T20:43:20.617839Z","iopub.status.idle":"2024-09-25T20:43:20.623064Z","shell.execute_reply.started":"2024-09-25T20:43:20.617802Z","shell.execute_reply":"2024-09-25T20:43:20.622072Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stdout","text":"rouge 1 similarity between lists: [0, 1] and [5, 5, 7, 0, 232] is 0.286\n","output_type":"stream"}]},{"cell_type":"code","source":"l1 = [1, 2, 3]\nl2 = [1, 2, 3, 4]\n\nr1s = rouge1_similarity(l1, l2)\n\nprint(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:21.108212Z","iopub.execute_input":"2024-09-25T20:43:21.109403Z","iopub.status.idle":"2024-09-25T20:43:21.115207Z","shell.execute_reply.started":"2024-09-25T20:43:21.109348Z","shell.execute_reply":"2024-09-25T20:43:21.114136Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Computing the overall score","metadata":{}},{"cell_type":"markdown","source":"# Average overlap","metadata":{}},{"cell_type":"code","source":"def average_overlap(samples, similarity_fn):\n    \n    scores = {}\n    \n    for index_candidate, candidate in enumerate(samples):\n        overlap = 0\n        \n        for index_sample, sample in enumerate(samples):\n            \n            if index_candidate == index_sample:\n                continue\n                \n            overlap += similarity_fn(candidate, sample)\n            \n        score = overlap / (len(samples) - 1)\n        \n        score = round(score, 3)\n        \n        scores[index_candidate] = score\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:22.116558Z","iopub.execute_input":"2024-09-25T20:43:22.117224Z","iopub.status.idle":"2024-09-25T20:43:22.123424Z","shell.execute_reply.started":"2024-09-25T20:43:22.117169Z","shell.execute_reply":"2024-09-25T20:43:22.122407Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"# Test with Jaccard similarity\n\nl1 = [1, 2, 3]\nl2 = [1, 2, 4]\nl3 = [1, 2, 4, 5]\n\navg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n\nprint(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:22.684583Z","iopub.execute_input":"2024-09-25T20:43:22.685529Z","iopub.status.idle":"2024-09-25T20:43:22.690965Z","shell.execute_reply.started":"2024-09-25T20:43:22.685490Z","shell.execute_reply":"2024-09-25T20:43:22.690082Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n\n{0: 0.45, 1: 0.625, 2: 0.575}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test with Rouge1 similarity\n\nl1 = [1, 2, 3]\nl2 = [1, 4]\nl3 = [1, 2, 4, 5]\nl4 = [5,6]\n\navg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n\nprint(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:23.238229Z","iopub.execute_input":"2024-09-25T20:43:23.238999Z","iopub.status.idle":"2024-09-25T20:43:23.245247Z","shell.execute_reply.started":"2024-09-25T20:43:23.238961Z","shell.execute_reply":"2024-09-25T20:43:23.244088Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n\n{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n","output_type":"stream"}]},{"cell_type":"code","source":"def weighted_avg_overlap(samples, log_probs, similarity_fn):\n    scores = {}\n    \n    for index_candidate, candidate in enumerate(samples):\n        overlap, weighted_sum = 0.0, 0.0\n        \n        for index_sample, (sample, logprob) in enumerate(zip(samples, log_probs)):\n            if index_candidate == index_sample:\n                continue\n                \n            sample_prob = float(np.exp(logprob))\n            weighted_sum += sample_prob\n            \n            sample_overlap = similarity_fn(candidate, sample)\n            overlap += sample_overlap * sample_prob\n            \n        score = overlap / weighted_sum\n        score = round(score, 3)\n        \n        scores[index_candidate] = score\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:23.569434Z","iopub.execute_input":"2024-09-25T20:43:23.569840Z","iopub.status.idle":"2024-09-25T20:43:23.576957Z","shell.execute_reply.started":"2024-09-25T20:43:23.569804Z","shell.execute_reply":"2024-09-25T20:43:23.575936Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"l1 = [1, 2, 3]\nl2 = [1, 2, 4]\nl3 = [1, 2, 4, 5]\nlog_probs = [0.4, 0.2, 0.5]\n\nw_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n\nprint(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:24.028455Z","iopub.execute_input":"2024-09-25T20:43:24.029665Z","iopub.status.idle":"2024-09-25T20:43:24.035780Z","shell.execute_reply.started":"2024-09-25T20:43:24.029609Z","shell.execute_reply":"2024-09-25T20:43:24.034713Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"weighted average overlap using Jaccard similarity is:\n\n{0: 0.443, 1: 0.631, 2: 0.558}\n","output_type":"stream"}]},{"cell_type":"code","source":"def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n    \n    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n    \n    decoded_translations = [ids_to_text(sample,tokenizer_por) for sample in samples]\n    \n    max_score_key = max(scores, key=lambda k: scores[k])\n    \n    translation = decoded_translations[max_score_key]\n    \n    return translation, decoded_translations","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:24.627029Z","iopub.execute_input":"2024-09-25T20:43:24.628009Z","iopub.status.idle":"2024-09-25T20:43:24.634210Z","shell.execute_reply.started":"2024-09-25T20:43:24.627966Z","shell.execute_reply":"2024-09-25T20:43:24.633233Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"english_sentence = \"I love languages\"\n\ntranslation, candidates = mbr_decode(translator, english_sentence, n_samples=10, temperature=0.6)\n\nprint(\"Translation candidates:\")\nfor c in candidates:\n    print(c)\n\nprint(f\"\\nSelected translation: {translation}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:43:25.068077Z","iopub.execute_input":"2024-09-25T20:43:25.068964Z","iopub.status.idle":"2024-09-25T20:43:26.547488Z","shell.execute_reply.started":"2024-09-25T20:43:25.068927Z","shell.execute_reply":"2024-09-25T20:43:26.546382Z"},"trusted":true},"execution_count":162,"outputs":[{"name":"stdout","text":"Translation candidates:\n['pareceu defender aprovou amputou zangado minhas propriedade cinico mandam idade afeccoes fracassou culpados dividas caso rios assado automatica recebemos racista orgulhe cansaco proibido surpresas brincarem virou incompetente acordalo responsavel seguiremos valor parava deslize espessa roubo acostuma sortudos opcional odiaria demissao precisas visita aprendizagem chovesse italianos aborreceu adicao brucos anotacoes experiencias']\n['convencelo escuras estabulo bibliotecaria rico fritando vivido esvazie madre preocupacoes fundamento zonzo possibilidades machucaria diabetico ignorou oportunidades bandagem cancelado ensaiar acalmaram cobraram abraca dados croissant nevar mundial adicionar concluido corrigir casando mantem empresarios cuida agenda trabalhos costumo bronzeado daninhas mexer panama secou vi pro precisares desinteressado festejava ditador prestem cabelo']\n['preta croissant prisional salva identificaram sobreviver exposto reais aberta preferida folga padre levantemse segundos noite madrasta reciclar renunciar preparando postas tudo indigenas esquentando forcou candidato idiota terreno sono embaixada guindaste calculo torcicolo caidas alemao peixes ultrapassou figura causado bancar passagem milionario dira sarcastico bate ferias sentisse escalada esfriando colmeias sabera']\n['confiavel brasileira pilotar detetives roleta continuo voo suprimento escutou ganhara amizade avisalo barreira genuino chora empurrei ora comera respeitar disparado colesterol lembremse acamado sinais sao imposto gritado aumenta empresto pousou chata inaceitavel cavalheiro anotei criada tentaram considerado apressate tirada vire achalo pular gripe taxi preocupada satisfeito ciumes estupidas considera darwin']\n['esfregao objetivos autoestrada corra concorrer elogio ofereceram mostramos aparecera surpreenderam semanalmente diminuir acougueiro mil podemse compreilhe centigrado independencia colonia cobraram queimada recepcao direi insensivel gaste doa comemoram agradecerlhe percebido divirtase escrito precisei bemvindos precisara ninar ajudalas entrega historias cobertores sorriso sermao nascer aranhas sonhava sentindo bomsenso agindo estudara sabiamente colher']\n['religioso brava trabalham decisao prometeu sobre pretendendo amanha oportunidade pagaremos besteiras enrolar sal agencia mosca soube limao reggae solte astro vinho mexicana fina gramaticais escola velhos retornar pessoa rejeitou escaparam graca sementes incentivou fraca africanos banheiro considera excecao seguida exatamente salvar marrom panama festa departamentos hora bruto aposento interno sentiriam']\n['japonesa opoe esposas garantiu mamifero morar inimigos frances sofri cercado conta esponja pulmao pastor numeros recebemos aposento voluntario barrado rolou ambiental merecem gasta amarre caminhemos felizmente chuvoso fardo final recepcao ortografia dera faloei discute relacoes comparar levo diferenca escondemos desgraca sabias duplo imigrou festa john misture tricotando meio pendurar abaco']\n['psicologia mito matriculou agente mama impressionalo minutinhos acoes estiver avisa iluda vendendo seria acalme desejamos remunerado abalado chef funcionarios deprimente periferia desligar estiveram ciente param chicago pobres tensa ligada comediante tristes queixas aborrecer quarto esbarrei exibindo criativa virus toca enterrou adversario expresso entrarem raiva trazme pai pede estrangeiros mostraram feri']\n['pregou acesso venderam acreditei trator amordacado otima vaga prego fixo pudim cafeteria lentamente potente vaso drasticamente descascar estorias idosas mamae mexicana vendese reais canta conexao qualidade novas freira semana artistas feriados experimento conclui feriu escrevam pegando vir estudasse restam seguintes racionais adiantada pisar dobrar iphone caminho ursos zombar latim levantei']\n['tapetes voluntarios horrivel adverbio sr nato tornarsea incorreta pressionar ventando argumento estagiarios choram americanos diploma delas serei convenci atuacao cremado intencional ajudassemos espere crocodilo aventure divertindo apta suicos aposta seriam choveu venceremos sentamos baixaram substituiram aleijeime noiva andar aquecendo mate incrivel desculpado matado proteger escreveria tornouse estejam tomara gelada beijaram']\n\nSelected translation: ['religioso brava trabalham decisao prometeu sobre pretendendo amanha oportunidade pagaremos besteiras enrolar sal agencia mosca soube limao reggae solte astro vinho mexicana fina gramaticais escola velhos retornar pessoa rejeitou escaparam graca sementes incentivou fraca africanos banheiro considera excecao seguida exatamente salvar marrom panama festa departamentos hora bruto aposento interno sentiriam']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
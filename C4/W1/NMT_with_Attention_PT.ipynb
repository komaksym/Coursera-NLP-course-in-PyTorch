{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9617371,"sourceType":"datasetVersion","datasetId":5754974}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cd /kaggle/input/dataset-eng-por","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:29:19.373741Z","iopub.execute_input":"2024-10-13T20:29:19.374122Z","iopub.status.idle":"2024-10-13T20:29:19.387492Z","shell.execute_reply.started":"2024-10-13T20:29:19.374083Z","shell.execute_reply":"2024-10-13T20:29:19.386570Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-eng-por\n","output_type":"stream"}]},{"cell_type":"code","source":"import pdb\nimport torch\nimport itertools\nimport numpy as np\nimport torch.nn as nn\nfrom collections import Counter\nfrom utils_PT import (sentences, train_dataset, val_dataset, train_loader, val_loader,\n                   tokenizer_eng, tokenizer_por, masked_loss, masked_acc, ids_to_text, encode_sample, pt_lower_and_split_punct)","metadata":{"id":"7GLdPiE-GQNX","execution":{"iopub.status.busy":"2024-10-13T20:29:19.389260Z","iopub.execute_input":"2024-10-13T20:29:19.389702Z","iopub.status.idle":"2024-10-13T20:29:32.997267Z","shell.execute_reply.started":"2024-10-13T20:29:19.389652Z","shell.execute_reply":"2024-10-13T20:29:32.996458Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"_sgfznSZWFvo","execution":{"iopub.status.busy":"2024-10-13T20:29:32.998407Z","iopub.execute_input":"2024-10-13T20:29:32.998820Z","iopub.status.idle":"2024-10-13T20:29:33.032007Z","shell.execute_reply.started":"2024-10-13T20:29:32.998787Z","shell.execute_reply":"2024-10-13T20:29:33.031065Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{"id":"OxlGeg_dRr5F"}},{"cell_type":"code","source":"english_sentences, portuguese_sentences = sentences\n\nprint(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\nprint(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")","metadata":{"id":"IKqu8oXERRRm","outputId":"dfa12300-c374-49ae-bea2-c9621abfc330","execution":{"iopub.status.busy":"2024-10-13T20:29:33.034629Z","iopub.execute_input":"2024-10-13T20:29:33.035018Z","iopub.status.idle":"2024-10-13T20:29:33.043670Z","shell.execute_reply.started":"2024-10-13T20:29:33.034976Z","shell.execute_reply":"2024-10-13T20:29:33.042795Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"English (to translate) sentence:\n\nNo matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n\nPortuguese (translation) sentence:\n\nNão importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n","output_type":"stream"}]},{"cell_type":"code","source":"source = pt_lower_and_split_punct(english_sentences[-5])\nprint(f\"source: {source}\\n\")\n\nencoded = tokenizer_eng.encode(source[0]).ids\nprint(f\"encoded: {encoded}\\n\")\nprint(f\"len(encoded): {len(encoded)}\")\n\ndecoded = tokenizer_eng.decode(encoded)\nprint(f\"decoded: {decoded}\\n\")\nprint(f\"len(decoded): {len(decoded)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:29:33.044759Z","iopub.execute_input":"2024-10-13T20:29:33.045076Z","iopub.status.idle":"2024-10-13T20:29:33.054325Z","shell.execute_reply.started":"2024-10-13T20:29:33.045034Z","shell.execute_reply":"2024-10-13T20:29:33.053533Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"source: ['[SOS] no matter how much you try to convince people that chocolate is vanilla ,  itll still be chocolate ,  even though you may manage to convince yourself and a few others that its vanilla . [EOS]']\n\nencoded: [3, 89, 473, 49, 106, 8, 220, 7, 1060, 135, 13, 1107, 12, 4151, 24, 502, 103, 33, 1107, 24, 251, 1169, 8, 219, 2558, 7, 1060, 344, 43, 11, 365, 791, 13, 52, 4151, 4, 2]\n\nlen(encoded): 37\ndecoded: [SOS] no matter how much you try to convince people that chocolate is vanilla , itll still be chocolate , even though you may manage to convince yourself and a few others that its vanilla . [EOS]\n\nlen(decoded): 195\n","output_type":"stream"}]},{"cell_type":"code","source":"del portuguese_sentences\ndel english_sentences\ndel sentences","metadata":{"id":"P5_KmRiKmiAt","execution":{"iopub.status.busy":"2024-10-13T20:29:33.055636Z","iopub.execute_input":"2024-10-13T20:29:33.055918Z","iopub.status.idle":"2024-10-13T20:29:33.063929Z","shell.execute_reply.started":"2024-10-13T20:29:33.055888Z","shell.execute_reply":"2024-10-13T20:29:33.063086Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ten_words_eng_vocab = sorted(tokenizer_eng.get_vocab().items(), key=lambda item: item[1])[:10]\nten_words_por_vocab = sorted(tokenizer_por.get_vocab().items(), key=lambda item: item[1])[:10]\nprint(f\"First 10 words of the english vocabulary:\\n\\n{[token[0] for token in ten_words_eng_vocab]}\\n\")\nprint(f\"First 10 words of the portuguese vocabulary:\\n\\n{[token[0] for token in ten_words_por_vocab]}\")","metadata":{"id":"xzQtQPDzlYOH","outputId":"a5bf37d7-a341-4f8b-eecb-f0494f18edb7","execution":{"iopub.status.busy":"2024-10-13T20:29:33.064921Z","iopub.execute_input":"2024-10-13T20:29:33.065210Z","iopub.status.idle":"2024-10-13T20:29:33.122576Z","shell.execute_reply.started":"2024-10-13T20:29:33.065180Z","shell.execute_reply":"2024-10-13T20:29:33.121530Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"First 10 words of the english vocabulary:\n\n['[PAD]', '[UNK]', '[EOS]', '[SOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n\nFirst 10 words of the portuguese vocabulary:\n\n['[PAD]', '[UNK]', '[EOS]', '[SOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Size of the vocabulary\nvocab_size_por = tokenizer_eng.get_vocab_size()\nvocab_size_eng = tokenizer_eng.get_vocab_size()\n\nprint(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\nprint(f\"English vocabulary is made up of {vocab_size_eng} words\")","metadata":{"id":"VMMIOTCso5GG","outputId":"960224f9-8fd5-4d34-98bb-7565d2b0403f","execution":{"iopub.status.busy":"2024-10-13T20:29:33.123679Z","iopub.execute_input":"2024-10-13T20:29:33.124028Z","iopub.status.idle":"2024-10-13T20:29:33.132341Z","shell.execute_reply.started":"2024-10-13T20:29:33.123996Z","shell.execute_reply":"2024-10-13T20:29:33.131490Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Portuguese vocabulary is made up of 12000 words\nEnglish vocabulary is made up of 12000 words\n","output_type":"stream"}]},{"cell_type":"code","source":"def word_to_id(token):\n    return tokenizer_por.token_to_id(token)\n\n\ndef ids_to_words(id):\n    return tokenizer_por.id_to_token(id)","metadata":{"id":"-KnZE672xXqm","execution":{"iopub.status.busy":"2024-10-13T20:29:33.133870Z","iopub.execute_input":"2024-10-13T20:29:33.134285Z","iopub.status.idle":"2024-10-13T20:29:33.141107Z","shell.execute_reply.started":"2024-10-13T20:29:33.134238Z","shell.execute_reply":"2024-10-13T20:29:33.140344Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"unk_id = word_to_id(\"[UNK]\")\nsos_id = word_to_id(\"[SOS]\")\neos_id = word_to_id(\"[EOS]\")\nbaunilha_id = word_to_id(\"baunilha\")\n\nprint(f\"The id for the [UNK] token is {unk_id}\")\nprint(f\"The id for the [SOS] token is {sos_id}\")\nprint(f\"The id for the [EOS] token is {eos_id}\")\nprint(f\"The id for baunilha (vanilla) is {baunilha_id}\")","metadata":{"id":"UFj-6OSuxYFG","outputId":"68606c95-da88-4c5d-946a-08ae17eba943","execution":{"iopub.status.busy":"2024-10-13T20:29:33.144635Z","iopub.execute_input":"2024-10-13T20:29:33.144997Z","iopub.status.idle":"2024-10-13T20:29:33.151282Z","shell.execute_reply.started":"2024-10-13T20:29:33.144966Z","shell.execute_reply":"2024-10-13T20:29:33.150435Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The id for the [UNK] token is 1\nThe id for the [SOS] token is 3\nThe id for the [EOS] token is 2\nThe id for baunilha (vanilla) is 5242\n","output_type":"stream"}]},{"cell_type":"code","source":"(to_translate, sr_translation), translation = next(iter(train_loader))\n\nprint(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\nprint(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\nprint(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")\n\nprint(tokenizer_eng.decode(to_translate[0, :].numpy()))\nprint(tokenizer_por.decode(sr_translation[0, :].numpy()))\nprint(tokenizer_por.decode(translation[0, :].numpy()))","metadata":{"id":"HbLzXYW23nbW","outputId":"95723009-c945-4b36-98db-7851be5bf8ed","execution":{"iopub.status.busy":"2024-10-13T20:29:33.152639Z","iopub.execute_input":"2024-10-13T20:29:33.153043Z","iopub.status.idle":"2024-10-13T20:29:33.236549Z","shell.execute_reply.started":"2024-10-13T20:29:33.153002Z","shell.execute_reply":"2024-10-13T20:29:33.235649Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Tokenized english sentence:\n[   3  173   46   66  282   66   22 2167  793    4    2    0    0    0\n    0    0    0    0    0]\n\n\nTokenized portuguese sentence (shifted to the right):\n[  3 103 171   6  12 744 378   4   0   0   0   0   0   0   0   0   0   0\n   0]\n\n\nTokenized portuguese sentence:\n[103 171   6  12 744 378   4   2   0   0   0   0   0   0   0   0   0   0\n   0]\n\n\n[SOS] lets go as soon as it stops raining . [EOS]\n[SOS] vamos assim que a chuva parar .\nvamos assim que a chuva parar . [EOS]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Encoder","metadata":{"id":"R1vQncuf5WT1"}},{"cell_type":"code","source":"VOCAB_SIZE = 12000\nUNITS = 256","metadata":{"id":"3yrT4UUA5XWC","execution":{"iopub.status.busy":"2024-10-13T20:29:33.237792Z","iopub.execute_input":"2024-10-13T20:29:33.238281Z","iopub.status.idle":"2024-10-13T20:29:33.242519Z","shell.execute_reply.started":"2024-10-13T20:29:33.238238Z","shell.execute_reply":"2024-10-13T20:29:33.241558Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n        self.rnn = nn.LSTM(units, units, bidirectional=True, batch_first=True)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        # Summarizing the bidirectional RNNs to follow the TF version\n        forward_output = x[:, :, :UNITS]\n        backward_output = x[:, :, UNITS:]\n        x = forward_output + backward_output\n\n        return x","metadata":{"id":"1Dwpp8_eBF76","execution":{"iopub.status.busy":"2024-10-13T20:29:33.243856Z","iopub.execute_input":"2024-10-13T20:29:33.244150Z","iopub.status.idle":"2024-10-13T20:29:33.252185Z","shell.execute_reply.started":"2024-10-13T20:29:33.244120Z","shell.execute_reply":"2024-10-13T20:29:33.251420Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(VOCAB_SIZE, UNITS)\n\nencoder_output = encoder(to_translate)\n\nprint(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\nprint(f'Encoder output has shape: {encoder_output.shape}')","metadata":{"id":"coZ1glxoJiRv","outputId":"19e6e093-1d60-4dc2-9db4-a7f1533b4439","execution":{"iopub.status.busy":"2024-10-13T20:29:33.253232Z","iopub.execute_input":"2024-10-13T20:29:33.255565Z","iopub.status.idle":"2024-10-13T20:29:33.449443Z","shell.execute_reply.started":"2024-10-13T20:29:33.255502Z","shell.execute_reply":"2024-10-13T20:29:33.448497Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Tensor of sentences in english has shape: torch.Size([64, 19])\n\nEncoder output has shape: torch.Size([64, 19, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cross Attention","metadata":{"id":"SfI3vy3cUZ3Q"}},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, units):\n        super().__init__()\n\n        self.mha = nn.MultiheadAttention(units, 1, batch_first=True)\n        self.layernorm = nn.LayerNorm(units)\n\n    def forward(self, context, target):\n        attn_output = self.mha(query=target,key=context, value=context)\n        x = target + attn_output[0] # [0] because we only need the attention output and no weights\n        x = self.layernorm(x) \n\n        return x","metadata":{"id":"PkeZr7_vKe_w","execution":{"iopub.status.busy":"2024-10-13T20:29:33.450700Z","iopub.execute_input":"2024-10-13T20:29:33.451111Z","iopub.status.idle":"2024-10-13T20:29:33.458363Z","shell.execute_reply.started":"2024-10-13T20:29:33.451067Z","shell.execute_reply":"2024-10-13T20:29:33.457446Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"attention_layer = CrossAttention(UNITS)\n\nsr_translation_embed = nn.Embedding(VOCAB_SIZE, UNITS, 0)(sr_translation)\n\nattention_result = attention_layer(encoder_output, sr_translation_embed)\n\nprint(f'Tensor of contexts has shape: {encoder_output.shape}')\nprint(f'Tensor of translations has shape: {sr_translation_embed.shape}')\nprint(f'Tensor of attention scores has shape: {attention_result.shape}')","metadata":{"id":"-2gsNK36XfgJ","outputId":"0ef58385-ab82-4eda-cce7-cc59612aaa7e","execution":{"iopub.status.busy":"2024-10-13T20:29:33.459718Z","iopub.execute_input":"2024-10-13T20:29:33.460091Z","iopub.status.idle":"2024-10-13T20:29:33.567155Z","shell.execute_reply.started":"2024-10-13T20:29:33.460049Z","shell.execute_reply":"2024-10-13T20:29:33.566163Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Tensor of contexts has shape: torch.Size([64, 19, 256])\nTensor of translations has shape: torch.Size([64, 19, 256])\nTensor of attention scores has shape: torch.Size([64, 19, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Decoder","metadata":{"id":"4iuxMJx7cnB4"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, units, padding_idx=0)\n        self.pre_attention_rnn = nn.LSTM(units, units, batch_first=True)\n        self.attention = CrossAttention(units)\n        self.post_attention_rnn = nn.LSTM(units, units, batch_first=True)\n        self.output_layer = nn.Linear(units, vocab_size)\n        self.activation = nn.LogSoftmax(dim=-1)\n\n    def forward(self, context, target_in, state=None, return_state=False):\n        x = self.embedding(target_in)\n        x, (hidden_state, cell_state) = self.pre_attention_rnn(x, state)\n        x = self.attention(context, x)\n        x, _ = self.post_attention_rnn(x)\n        x = self.output_layer(x)\n        logits = self.activation(x)\n\n        if return_state:\n            return logits, [hidden_state, cell_state]\n\n        return logits","metadata":{"id":"9uaq8tR4cnmj","execution":{"iopub.status.busy":"2024-10-13T20:29:33.568457Z","iopub.execute_input":"2024-10-13T20:29:33.568856Z","iopub.status.idle":"2024-10-13T20:29:33.577330Z","shell.execute_reply.started":"2024-10-13T20:29:33.568813Z","shell.execute_reply":"2024-10-13T20:29:33.576281Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(VOCAB_SIZE, UNITS)\n\nlogits = decoder(encoder_output, sr_translation)\n\nprint(f'Tensor of contexts has shape: {encoder_output.shape}')\nprint(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\nprint(f'Tensor of logits has shape: {logits.shape}')","metadata":{"id":"GMVYa392xyjW","outputId":"e62da169-facb-4967-bb52-93987b6c368c","execution":{"iopub.status.busy":"2024-10-13T20:29:33.578648Z","iopub.execute_input":"2024-10-13T20:29:33.579199Z","iopub.status.idle":"2024-10-13T20:29:33.766708Z","shell.execute_reply.started":"2024-10-13T20:29:33.579153Z","shell.execute_reply":"2024-10-13T20:29:33.765681Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Tensor of contexts has shape: torch.Size([64, 19, 256])\nTensor of right-shifted translations has shape: torch.Size([64, 19])\nTensor of logits has shape: torch.Size([64, 19, 12000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Translator","metadata":{"id":"GPOfZIPYBNNu"}},{"cell_type":"code","source":"class Translator(nn.Module):\n    def __init__(self, vocab_size, units):\n        super().__init__()\n\n        self.encoder = Encoder(vocab_size, units)\n        self.decoder = Decoder(vocab_size, units)\n\n    def forward(self, inputs):\n        context, targets = inputs\n\n        encoded_context = self.encoder(context)\n        logits = self.decoder(encoded_context, targets)\n\n        return logits","metadata":{"id":"ghu6C60J1jD_","execution":{"iopub.status.busy":"2024-10-13T20:29:33.768013Z","iopub.execute_input":"2024-10-13T20:29:33.768388Z","iopub.status.idle":"2024-10-13T20:29:33.774566Z","shell.execute_reply.started":"2024-10-13T20:29:33.768354Z","shell.execute_reply":"2024-10-13T20:29:33.773475Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"translator = Translator(VOCAB_SIZE, UNITS).to(device)\n\n# Loading the model\n#translator.load_state_dict(torch.load('/kaggle/working/model_weights.pth', map_location=torch.device(device), weights_only=True))\n\nlogits = translator((to_translate.to(device), sr_translation.to(device)))\n\nprint(f'Tensor of sentences to translate has shape: {to_translate.shape}')\nprint(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\nprint(f'Tensor of logits has shape: {logits.shape}')","metadata":{"id":"Hxw4GurPGfsh","outputId":"f527ed49-df4c-418a-a921-3d7e6feb27d2","execution":{"iopub.status.busy":"2024-10-13T20:29:33.775786Z","iopub.execute_input":"2024-10-13T20:29:33.776158Z","iopub.status.idle":"2024-10-13T20:29:34.436434Z","shell.execute_reply.started":"2024-10-13T20:29:33.776126Z","shell.execute_reply":"2024-10-13T20:29:34.435513Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Tensor of sentences to translate has shape: torch.Size([64, 19])\nTensor of right-shifted translations has shape: torch.Size([64, 19])\nTensor of logits has shape: torch.Size([64, 19, 12000])\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=translator.parameters())\ncriterion = masked_loss\nacc = masked_acc","metadata":{"id":"Kysqf7SpPg0B","execution":{"iopub.status.busy":"2024-10-13T20:29:34.437751Z","iopub.execute_input":"2024-10-13T20:29:34.438160Z","iopub.status.idle":"2024-10-13T20:29:35.225716Z","shell.execute_reply.started":"2024-10-13T20:29:34.438117Z","shell.execute_reply":"2024-10-13T20:29:35.224715Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"H9sDgrl3HGvD"}},{"cell_type":"code","source":"NUM_EPOCHS = 20\nSTEPS_PER_EPOCH = 500\nVALIDATION_STEPS = 50\npatience = 3\nbest_loss = float('inf')\nnum_batches_train = len(train_loader)\nnum_batches_val = len(val_loader)\n\n\nfor epoch in range(NUM_EPOCHS):\n    translator.train()\n    \n    # Mini batch loss\n    running_loss_train = 0.0\n    running_accuracy_train = 0.0\n    running_loss_val = 0.0\n    running_accuracy_val = 0.0\n    \n    # Using itertools for fixed length iteration over non subscriptable DataLoader\n    for i, data in enumerate(itertools.islice(train_loader,  STEPS_PER_EPOCH)):\n        (context, target_in), target_out = data\n        \n        context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n\n        optimizer.zero_grad()\n        outputs = translator((context, target_in))\n        loss = criterion(target_out, outputs)\n        accuracy = acc(target_out, outputs)\n        loss.backward()\n        optimizer.step()\n\n        running_loss_train += loss.item()\n        running_accuracy_train = accuracy \n        \n    \n    # Validation\n    translator.eval()\n    with torch.no_grad():\n        for i, data in enumerate(itertools.islice(val_loader,  VALIDATION_STEPS)):\n            (context, target_in), target_out = data\n\n            context, target_in, target_out = context.to(device), target_in.to(device), target_out.to(device)\n\n            outputs = translator((context, target_in))\n            loss = criterion(target_out, outputs)\n            accuracy = acc(target_out, outputs)\n\n            running_loss_val += loss.item()\n            running_accuracy_val = accuracy \n            \n            \n    # Print the data\n    print(f\"\\n[epoch: {epoch+1}/{NUM_EPOCHS}] masked_loss: {(running_loss_train / num_batches_train):.4f}, masked_acc: {running_accuracy_train:.4f}, val_masked_loss: {(running_loss_val / num_batches_val):.4f}, val_masked_acc: {running_accuracy_val:.4f}\\n\")\n    \n    # Update the best loss if it's better than the previous one\n    if running_loss_train < best_loss:\n        best_loss = running_loss_train\n        patience = 3\n\n    else:\n        # Losing patience\n        patience -= 1\n\n        if patience == 0:\n            print(\"Early stopping was triggered\")","metadata":{"id":"FUYcGoGOHHVS","_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-10-13T20:29:35.227025Z","iopub.execute_input":"2024-10-13T20:29:35.227426Z","iopub.status.idle":"2024-10-13T20:33:15.326418Z","shell.execute_reply.started":"2024-10-13T20:29:35.227394Z","shell.execute_reply":"2024-10-13T20:33:15.325442Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\n[epoch: 1/20] masked_loss: 0.8821, masked_acc: 0.5078, val_masked_loss: 0.2637, val_masked_acc: 0.5397\n\n\n[epoch: 2/20] masked_loss: 0.5508, masked_acc: 0.5787, val_masked_loss: 0.1937, val_masked_acc: 0.6485\n\n\n[epoch: 3/20] masked_loss: 0.4207, masked_acc: 0.6913, val_masked_loss: 0.1574, val_masked_acc: 0.6841\n\n\n[epoch: 4/20] masked_loss: 0.3460, masked_acc: 0.7121, val_masked_loss: 0.1379, val_masked_acc: 0.7218\n\n\n[epoch: 5/20] masked_loss: 0.3029, masked_acc: 0.7320, val_masked_loss: 0.1252, val_masked_acc: 0.7490\n\n\n[epoch: 6/20] masked_loss: 0.2720, masked_acc: 0.7563, val_masked_loss: 0.1156, val_masked_acc: 0.7657\n\n\n[epoch: 7/20] masked_loss: 0.2447, masked_acc: 0.7901, val_masked_loss: 0.1107, val_masked_acc: 0.7531\n\n\n[epoch: 8/20] masked_loss: 0.2250, masked_acc: 0.7753, val_masked_loss: 0.1050, val_masked_acc: 0.7594\n\n\n[epoch: 9/20] masked_loss: 0.2137, masked_acc: 0.8035, val_masked_loss: 0.1016, val_masked_acc: 0.7699\n\n\n[epoch: 10/20] masked_loss: 0.1966, masked_acc: 0.7647, val_masked_loss: 0.0983, val_masked_acc: 0.7762\n\n\n[epoch: 11/20] masked_loss: 0.1885, masked_acc: 0.8197, val_masked_loss: 0.0958, val_masked_acc: 0.7720\n\n\n[epoch: 12/20] masked_loss: 0.1807, masked_acc: 0.7820, val_masked_loss: 0.0942, val_masked_acc: 0.7845\n\n\n[epoch: 13/20] masked_loss: 0.1707, masked_acc: 0.7941, val_masked_loss: 0.0932, val_masked_acc: 0.7845\n\n\n[epoch: 14/20] masked_loss: 0.1653, masked_acc: 0.8071, val_masked_loss: 0.0914, val_masked_acc: 0.7678\n\n\n[epoch: 15/20] masked_loss: 0.1605, masked_acc: 0.8154, val_masked_loss: 0.0902, val_masked_acc: 0.7657\n\n\n[epoch: 16/20] masked_loss: 0.1543, masked_acc: 0.8571, val_masked_loss: 0.0896, val_masked_acc: 0.7678\n\n\n[epoch: 17/20] masked_loss: 0.1480, masked_acc: 0.8623, val_masked_loss: 0.0886, val_masked_acc: 0.7887\n\n\n[epoch: 18/20] masked_loss: 0.1444, masked_acc: 0.8129, val_masked_loss: 0.0879, val_masked_acc: 0.7845\n\n\n[epoch: 19/20] masked_loss: 0.1396, masked_acc: 0.8495, val_masked_loss: 0.0881, val_masked_acc: 0.7845\n\n\n[epoch: 20/20] masked_loss: 0.1376, masked_acc: 0.8290, val_masked_loss: 0.0873, val_masked_acc: 0.7992\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#translator.load_state_dict(torch.load('/kaggle/working/model_state_dict.pth', weights_only=True))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:15.327648Z","iopub.execute_input":"2024-10-13T20:33:15.327968Z","iopub.status.idle":"2024-10-13T20:33:15.332034Z","shell.execute_reply.started":"2024-10-13T20:33:15.327936Z","shell.execute_reply":"2024-10-13T20:33:15.331075Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Using the model for inference","metadata":{"id":"Nat9JkDJT8Tg"}},{"cell_type":"code","source":"def generate_next_token(context, decoder, next_token, state, done, temperature=0.0):\n    logits, state = decoder(context, next_token, state, return_state=True)\n    logits = logits[:, -1, :]\n\n    if temperature == 0.0:\n        next_token = torch.argmax(logits, dim=-1)\n\n    else:\n        logits = torch.exp(logits)\n        logits /= temperature\n        next_token = torch.multinomial(logits, 1)\n        logits = torch.log(logits)\n\n    logits = torch.squeeze(logits)\n\n    next_token = torch.squeeze(next_token)\n\n    logit = logits[next_token].detach().numpy()\n\n    next_token = torch.reshape(next_token, shape=(1,1))\n\n    if next_token == eos_id:\n        done = True\n\n    return next_token, logit, state, done","metadata":{"id":"dRVu0cp0T9vu","execution":{"iopub.status.busy":"2024-10-13T20:33:15.333226Z","iopub.execute_input":"2024-10-13T20:33:15.333963Z","iopub.status.idle":"2024-10-13T20:33:15.344834Z","shell.execute_reply.started":"2024-10-13T20:33:15.333920Z","shell.execute_reply":"2024-10-13T20:33:15.343855Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"eng_sentence = \"I love languages\"\n\ncontext = torch.tensor(encode_sample(eng_sentence))\ncontext = torch.unsqueeze(context, dim=0)\ncontext = encoder(context)\n\nnext_token = torch.full((1,1), sos_id)\n\nstate = [torch.rand((1, 1, UNITS)), torch.rand((1, 1, UNITS))]\ndone = False\n\nnext_token, logit, state, done = generate_next_token(context, decoder, next_token, state, done, temperature=0.5)\nprint(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")\nnext_token = next_token.tolist()","metadata":{"id":"E8UgIJZjD2_F","outputId":"0dc50d58-86b6-4f66-d31b-18051fd7f060","execution":{"iopub.status.busy":"2024-10-13T20:33:15.345866Z","iopub.execute_input":"2024-10-13T20:33:15.346153Z","iopub.status.idle":"2024-10-13T20:33:15.411110Z","shell.execute_reply.started":"2024-10-13T20:33:15.346123Z","shell.execute_reply":"2024-10-13T20:33:15.410263Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Next token: tensor([[5132]])\nLogit: -8.7250\nDone? False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Translate","metadata":{"id":"CDG_-BNy-knN"}},{"cell_type":"code","source":"def translate(model, text, max_length=50, temperature=0.0):\n    model.eval()\n    \n    tokens, logits = [], []\n\n    pre_text = text\n    text = torch.tensor(encode_sample(pre_text))[None, :]\n    context = model.encoder(text)\n\n    next_token = torch.full((1,1), sos_id)\n\n    state = [torch.zeros((1, 1, UNITS)), torch.zeros((1, 1, UNITS))]\n\n    done = False\n    for iteration in range(max_length):\n        try:\n            next_token, logit, state, done = generate_next_token(\n                context=context,\n                decoder=model.decoder,\n                next_token=next_token,\n                state=state,\n                done=done,\n                temperature=temperature\n            )\n        except:\n            raise Exception(\"Problem generating the next token\")\n\n        if done:\n            break\n            \n        tokens.append(next_token)\n        \n        logits.append(logit)\n\n    tokens = torch.cat(tokens, dim=-1).tolist()\n    \n    translation = ids_to_text(tokens, tokenizer_por)\n\n    return translation, logits[-1], tokens","metadata":{"id":"ZLqbKnLaxZ6J","execution":{"iopub.status.busy":"2024-10-13T20:33:15.412351Z","iopub.execute_input":"2024-10-13T20:33:15.412704Z","iopub.status.idle":"2024-10-13T20:33:15.421207Z","shell.execute_reply.started":"2024-10-13T20:33:15.412663Z","shell.execute_reply":"2024-10-13T20:33:15.420339Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Running this cell multiple times should return the same output since temp is 0\n\ntemp = 0.0\noriginal_sentence = \"I love languages\"\n\ntranslation, logit, tokens = translate(translator.to(\"cpu\"), original_sentence, temperature=temp)\n\nprint(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")","metadata":{"id":"dBKxYvlcjX-F","outputId":"8d5b1254-4c3f-40c1-8c56-0cace5a4282d","execution":{"iopub.status.busy":"2024-10-13T20:33:55.534757Z","iopub.execute_input":"2024-10-13T20:33:55.535588Z","iopub.status.idle":"2024-10-13T20:33:55.663439Z","shell.execute_reply.started":"2024-10-13T20:33:55.535548Z","shell.execute_reply":"2024-10-13T20:33:55.662432Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Temperature: 0.0\n\nOriginal sentence: I love languages\nTranslation: ['eu amo idiomas nelas notaram blog eu amo eu amo eu amo eu amo eu amo eu amo os eu amo eu amo eu amo eu amo eu amo eu amo amo eu amo os eu amo eu amo eu amo eu amo eu amo amo eu amo amo eu']\nTranslation tokens:[[9, 523, 888, 4761, 7783, 4009, 9, 523, 9, 523, 9, 523, 9, 523, 9, 523, 9, 523, 40, 9, 523, 9, 523, 9, 523, 9, 523, 9, 523, 9, 523, 523, 9, 523, 40, 9, 523, 9, 523, 9, 523, 9, 523, 9, 523, 523, 9, 523, 523, 9]]\nLogit: -1.025\n","output_type":"stream"}]},{"cell_type":"code","source":"# Running this cell multiple times should return different outputs since temp is not 0\n# You can try different temperatures\n\ntemp = 0.7\noriginal_sentence = \"I love languages\"\n\ntranslation, logit, tokens = translate(translator.to(\"cpu\"), original_sentence, temperature=temp)\n\nprint(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:52.832290Z","iopub.execute_input":"2024-10-13T20:33:52.832660Z","iopub.status.idle":"2024-10-13T20:33:52.974403Z","shell.execute_reply.started":"2024-10-13T20:33:52.832624Z","shell.execute_reply":"2024-10-13T20:33:52.973457Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Temperature: 0.7\n\nOriginal sentence: I love languages\nTranslation: ['eu amo cantame embriagada templos sofa sonho eu seu viajo meu tanto devo eu adoro arroz alteracoes eu brasil pus gordinho eu amo que pretendemos eu adoro anoitecer eu adoro gritos ridicula eu amo sensivel tarde trabalho encontrei perdi completamente desorientado esperarei eu eu amo estadunidenses destas nossa ostras boi']\nTranslation tokens:[[9, 523, 11801, 6204, 11065, 1065, 902, 9, 49, 5554, 43, 290, 400, 9, 537, 1524, 5580, 9, 1843, 5114, 10286, 9, 523, 6, 5923, 9, 537, 4893, 9, 537, 7641, 4562, 9, 523, 3182, 193, 143, 470, 555, 749, 7486, 3602, 9, 9, 523, 6237, 5705, 304, 6384, 2699]]\nLogit: -6.641\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Minimum Bayes-Risk Decoding","metadata":{}},{"cell_type":"code","source":"def generate_samples(model, text, n_samples=4, temperature=0.6):\n    samples, log_probs = [], []\n    \n    for _ in range(n_samples):\n        _, log_prob, sample = translate(model, text, temperature=temperature)\n        \n        samples.append(sample)\n        \n        log_probs.append(log_prob)\n        \n    return samples, log_probs","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:15.770910Z","iopub.execute_input":"2024-10-13T20:33:15.771234Z","iopub.status.idle":"2024-10-13T20:33:15.776866Z","shell.execute_reply.started":"2024-10-13T20:33:15.771199Z","shell.execute_reply":"2024-10-13T20:33:15.776108Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"samples, log_probs = generate_samples(translator, 'I love languages')\n\nfor s, l in zip(samples, log_probs):\n    print(f\"Translated tensor: {s} has logit: {l:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:15.777819Z","iopub.execute_input":"2024-10-13T20:33:15.778715Z","iopub.status.idle":"2024-10-13T20:33:16.279144Z","shell.execute_reply.started":"2024-10-13T20:33:15.778677Z","shell.execute_reply":"2024-10-13T20:33:16.278148Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Translated tensor: [[9, 523, 778, 11589, 6808, 8804, 60, 99, 7, 523, 9, 537, 8981, 9, 523, 5597, 9, 523, 5673, 9, 892, 159, 37, 7397, 1055, 9, 771, 7981, 461, 99, 5825, 9, 3607, 6576, 244, 9, 840, 5431, 1633, 155, 10716, 9, 523, 3165, 902, 257, 9, 537, 5545, 283]] has logit: -4.854\nTranslated tensor: [[9, 537, 5866, 4988, 319, 245, 9416, 1916, 2047, 416, 531, 49, 1, 9, 537, 7212, 1288, 5082, 9, 1145, 464, 7674, 807, 4377, 1, 2805, 229, 4698, 9, 99, 224, 9448, 183, 155, 2213, 216, 3517, 523, 9, 10439, 9, 5394, 3808, 8929, 8488, 840, 4048, 4728, 93, 3753]] has logit: -6.767\nTranslated tensor: [[9, 6835, 1, 5068, 248, 8646, 523, 9, 9, 523, 9, 306, 4, 1, 40, 3346, 9, 537, 7365, 9, 523, 6144, 1055, 4860, 1410, 6062, 775, 43, 11747, 9, 99, 6503, 3901, 9759, 3261, 1308, 2529, 2153, 647, 4810, 9, 416, 1882, 5756, 9, 523, 1334, 2047, 10642, 8211]] has logit: -7.727\nTranslated tensor: [[9, 523, 4939, 6704, 8821, 2418, 9, 523, 523, 99, 10370, 8170, 523, 268, 1397, 2441, 537, 5633, 3029, 9, 523, 4401, 3175, 9, 537, 1363, 196, 9, 55, 9, 3680, 7419, 10370, 7674, 60, 9, 1586, 523, 5590, 6162, 1220, 4024, 11421, 218, 4865, 348, 9, 9, 523, 9963]] has logit: -8.123\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Comparing overlaps","metadata":{}},{"cell_type":"code","source":"def jaccard_similarity(candidate, reference):\n    \n    if (isinstance(candidate, list) and all(isinstance(i, list) for i in candidate)) and \\\n       (isinstance(reference, list) and all(isinstance(i, list) for i in reference)):\n        candidate_set = set(candidate[0])\n        reference_set = set(reference[0])\n\n    else:\n        candidate_set = set(candidate)\n        reference_set = set(reference)    \n    \n    common_tokens = candidate_set.intersection(reference_set)\n    \n    all_tokens = candidate_set.union(reference_set)\n    \n    overlap = len(common_tokens) / len(all_tokens)\n    \n    return overlap","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.280442Z","iopub.execute_input":"2024-10-13T20:33:16.280859Z","iopub.status.idle":"2024-10-13T20:33:16.288245Z","shell.execute_reply.started":"2024-10-13T20:33:16.280815Z","shell.execute_reply":"2024-10-13T20:33:16.287103Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"l1 = [1,2,3]\nl2 = [1,2,3,4]\n\njs = jaccard_similarity(l1, l2)\n\nprint(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.289548Z","iopub.execute_input":"2024-10-13T20:33:16.290035Z","iopub.status.idle":"2024-10-13T20:33:16.299080Z","shell.execute_reply.started":"2024-10-13T20:33:16.289993Z","shell.execute_reply":"2024-10-13T20:33:16.298008Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Rouge1 similarity","metadata":{}},{"cell_type":"code","source":"def rouge1_similarity(candidate, reference):\n    candidate_word_counts = Counter(candidate)\n    reference_word_counts = Counter(reference)    \n    \n    overlap = 0\n    \n    for token in candidate_word_counts.keys():\n        token_count_candidate = candidate_word_counts[token]\n        token_count_reference = reference_word_counts[token]        \n        \n        overlap += min(token_count_candidate, token_count_reference)\n        \n    precision = overlap / len(candidate)\n    \n    recall = overlap / len(reference)\n    \n    if precision + recall != 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score\n    \n    return 0","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.300166Z","iopub.execute_input":"2024-10-13T20:33:16.300468Z","iopub.status.idle":"2024-10-13T20:33:16.309987Z","shell.execute_reply.started":"2024-10-13T20:33:16.300428Z","shell.execute_reply":"2024-10-13T20:33:16.309147Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"l1 = [0, 1]\nl2 = [5, 5, 7, 0, 232]\n\nr1s = rouge1_similarity(l1, l2)\n\nprint(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.311325Z","iopub.execute_input":"2024-10-13T20:33:16.311651Z","iopub.status.idle":"2024-10-13T20:33:16.319676Z","shell.execute_reply.started":"2024-10-13T20:33:16.311620Z","shell.execute_reply":"2024-10-13T20:33:16.318433Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"rouge 1 similarity between lists: [0, 1] and [5, 5, 7, 0, 232] is 0.286\n","output_type":"stream"}]},{"cell_type":"code","source":"l1 = [1, 2, 3]\nl2 = [1, 2, 3, 4]\n\nr1s = rouge1_similarity(l1, l2)\n\nprint(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.320721Z","iopub.execute_input":"2024-10-13T20:33:16.321026Z","iopub.status.idle":"2024-10-13T20:33:16.337068Z","shell.execute_reply.started":"2024-10-13T20:33:16.320996Z","shell.execute_reply":"2024-10-13T20:33:16.336165Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Computing the overall score","metadata":{}},{"cell_type":"markdown","source":"# Average overlap","metadata":{}},{"cell_type":"code","source":"def average_overlap(samples, similarity_fn):\n    \n    scores = {}\n    \n    for index_candidate, candidate in enumerate(samples):\n        overlap = 0\n        \n        for index_sample, sample in enumerate(samples):\n            \n            if index_candidate == index_sample:\n                continue\n                \n            overlap += similarity_fn(candidate, sample)\n            \n        score = overlap / (len(samples) - 1)\n        \n        score = round(score, 3)\n        \n        scores[index_candidate] = score\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.338110Z","iopub.execute_input":"2024-10-13T20:33:16.338401Z","iopub.status.idle":"2024-10-13T20:33:16.347248Z","shell.execute_reply.started":"2024-10-13T20:33:16.338369Z","shell.execute_reply":"2024-10-13T20:33:16.346500Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Test with Jaccard similarity\n\nl1 = [1, 2, 3]\nl2 = [1, 2, 4]\nl3 = [1, 2, 4, 5]\n\navg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n\nprint(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.348425Z","iopub.execute_input":"2024-10-13T20:33:16.349033Z","iopub.status.idle":"2024-10-13T20:33:16.357719Z","shell.execute_reply.started":"2024-10-13T20:33:16.349001Z","shell.execute_reply":"2024-10-13T20:33:16.356739Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n\n{0: 0.45, 1: 0.625, 2: 0.575}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test with Rouge1 similarity\n\nl1 = [1, 2, 3]\nl2 = [1, 4]\nl3 = [1, 2, 4, 5]\nl4 = [5,6]\n\navg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n\nprint(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.358881Z","iopub.execute_input":"2024-10-13T20:33:16.359634Z","iopub.status.idle":"2024-10-13T20:33:16.368429Z","shell.execute_reply.started":"2024-10-13T20:33:16.359593Z","shell.execute_reply":"2024-10-13T20:33:16.367487Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n\n{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n","output_type":"stream"}]},{"cell_type":"code","source":"def weighted_avg_overlap(samples, log_probs, similarity_fn):\n    scores = {}\n    \n    for index_candidate, candidate in enumerate(samples):\n        overlap, weighted_sum = 0.0, 0.0\n        \n        for index_sample, (sample, logprob) in enumerate(zip(samples, log_probs)):\n            if index_candidate == index_sample:\n                continue\n                \n            sample_prob = float(np.exp(logprob))\n            weighted_sum += sample_prob\n            \n            sample_overlap = similarity_fn(candidate, sample)\n            overlap += sample_overlap * sample_prob\n            \n        score = overlap / weighted_sum\n        score = round(score, 3)\n        \n        scores[index_candidate] = score\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.369368Z","iopub.execute_input":"2024-10-13T20:33:16.370202Z","iopub.status.idle":"2024-10-13T20:33:16.377798Z","shell.execute_reply.started":"2024-10-13T20:33:16.370159Z","shell.execute_reply":"2024-10-13T20:33:16.376972Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"l1 = [1, 2, 3]\nl2 = [1, 2, 4]\nl3 = [1, 2, 4, 5]\nlog_probs = [0.4, 0.2, 0.5]\n\nw_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n\nprint(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.379285Z","iopub.execute_input":"2024-10-13T20:33:16.379673Z","iopub.status.idle":"2024-10-13T20:33:16.392309Z","shell.execute_reply.started":"2024-10-13T20:33:16.379595Z","shell.execute_reply":"2024-10-13T20:33:16.391234Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"weighted average overlap using Jaccard similarity is:\n\n{0: 0.443, 1: 0.631, 2: 0.558}\n","output_type":"stream"}]},{"cell_type":"code","source":"def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n    \n    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n    \n    decoded_translations = [ids_to_text(sample,tokenizer_por) for sample in samples]\n    \n    max_score_key = max(scores, key=lambda k: scores[k])\n    \n    translation = decoded_translations[max_score_key]\n    \n    return translation, decoded_translations","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.393369Z","iopub.execute_input":"2024-10-13T20:33:16.393648Z","iopub.status.idle":"2024-10-13T20:33:16.403763Z","shell.execute_reply.started":"2024-10-13T20:33:16.393609Z","shell.execute_reply":"2024-10-13T20:33:16.403006Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"english_sentence = \"I love languages\"\n\ntranslation, candidates = mbr_decode(translator, english_sentence, n_samples=10, temperature=0.6)\n\nprint(\"Translation candidates:\")\nfor c in candidates:\n    print(c)\n\nprint(f\"\\nSelected translation: {translation}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:33:16.405079Z","iopub.execute_input":"2024-10-13T20:33:16.405672Z","iopub.status.idle":"2024-10-13T20:33:17.618252Z","shell.execute_reply.started":"2024-10-13T20:33:16.405633Z","shell.execute_reply":"2024-10-13T20:33:17.617223Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Translation candidates:\n['eu gosto serio fiquemos gatinhos departamento altas eu eu adoro cooperacao professores treze duzentos eu amo altura eu eu tratamento tanto abrilo eu jogo domingos na dedos eu adoro perda linguas logo choro reconheco eu eu gosto velejar acelerar malucos eu amo escolha eletrica ensino sempre amo eu adoro quantidades']\n['eu amo linguas garantir formularios vim discutir depois barato ontem eu adoro levado eu meu permitir altura eu amo meus droga eu adoro necessaria eu amo engracados eu realmente aprendi policiais brincar palavras cru comeco na gota principio eu jogo bancar surpresas grandes oro eu adoro meia eu amo tomates']\n['eu amo espirrar dez assinado eu lhe passaros eu velho amo eu perdidos acima eu chame animais eu eu amo muito demissao perguntava parar adoro cervos amo parei eu amo tomates decepcionado acordo surpresa as pedacos tanto cinco fofocar detesto eu meia muitos a meus rindo ganho trombei tentado asilo']\n['amo viagens projetado diferencas acho eu adoro halloween eu gosto detesta ronco escutar pico longo envenenamento eu adoro terei maratona eu eu adoro honestas visitas prioridades odeio sempre brinquedos apos humanos amo garagem eu amo tomei enxaqueca eu amo aposto eu falta amendoins compro eu adoro gangue oi escrevi hora']\n['eu meia ligacoes quando lisonjeado concordo ensino amei amo eu amo no honesto ame eu pretendo tentado turistas baixei errei telefone comprou manchas engracados acucar toco decisoes eu gosto ultimos cep provas eu jogo linguas damas bolsos conteme sempre imaginei esquecendo batalha visitei durante quadrinhos lista estudei animal eu eu']\n['eu amo planos inflacao contribuindo confie eu andei gasto eu gosto esquecime adoramos metade ver quero chamine tirou medalha aos animado intimo costumo inumeras meu maquiagem gostava eu adoro estadunidenses eu acordei desliguei picadas ensino comi obrigar dez cadeiras terras cabuqui entendo quando eu eu adorei surpresas limpando eu amoo']\n['eu amoo falhar conduta tiro diferencas restante meu comecei verdade eu adoro externo ate fumar juntos limite visitas voos guerra aposto eu falta demitida frequentemente eu eu adoro revisar toco roubei tres fio responsabilidades gosto halloween eu amo apertar gasto julguei sarro feios dadas eu eu bebo bancar encontrarem']\n['eu adoro arrependimentos com florestas assistimos eu eu adoro prestar honestos homens eu adoro desenhar receio pouquissimos programacao quanto meia eu odeio jogos bloco eu gosto mel hoje ate eu agradeco eu adoro adolescentes ando pouca conserto eu amo pausa simplesmente amor deduzo eu querida adoro espirituoso hospitais grosseiro hora']\n['eu amo torcendo mesmas consertado invejo eu percebo fiz meia sexta caminhadas eu tranquei durante linguas noivado muito eu passo nas dos tomates ligo linguas eu concordo prioridades estudado medidas intrigado onze proprio acucar dura levalos eu eu eu gosto epoca eu amo depois chute alugar amostra suas surpresas olhos']\n['eu adoro produtos biscoitos disparado eu amo potencial eu adoro vendese civis adoro rosas admiro sanduiches adoro enxaqueca eu amo cerejas aves os lembro nossa fora adoro interessantes devendo nivel eu adoro videogame quando eu eu amo silencio eu gosto muito trato achille espectador cabra danco costumo nestes linguas eu']\n\nSelected translation: ['eu amo torcendo mesmas consertado invejo eu percebo fiz meia sexta caminhadas eu tranquei durante linguas noivado muito eu passo nas dos tomates ligo linguas eu concordo prioridades estudado medidas intrigado onze proprio acucar dura levalos eu eu eu gosto epoca eu amo depois chute alugar amostra suas surpresas olhos']\n","output_type":"stream"}]}]}
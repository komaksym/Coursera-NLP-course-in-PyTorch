{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9516043,"sourceType":"datasetVersion","datasetId":5791338}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cd /kaggle/input/transformer-summarizer-ds","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:17.168564Z","iopub.execute_input":"2024-09-30T19:48:17.169059Z","iopub.status.idle":"2024-09-30T19:48:17.206481Z","shell.execute_reply.started":"2024-09-30T19:48:17.169012Z","shell.execute_reply":"2024-09-30T19:48:17.205154Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/transformer-summarizer-ds\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport pdb\nimport time\nimport utils\nimport torch\nimport textwrap\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nwrapper = textwrap.TextWrapper(width=70)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T19:48:17.245916Z","iopub.execute_input":"2024-09-30T19:48:17.246916Z","iopub.status.idle":"2024-09-30T19:48:21.882357Z","shell.execute_reply.started":"2024-09-30T19:48:17.246854Z","shell.execute_reply":"2024-09-30T19:48:21.881050Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/transformer-summarizer-ds/train.json\n/kaggle/input/transformer-summarizer-ds/test.json\n/kaggle/input/transformer-summarizer-ds/utils.py\n/kaggle/input/transformer-summarizer-ds/val.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import the Dataset","metadata":{}},{"cell_type":"code","source":"train_data, test_data = utils.get_train_test_data()\n\n# A single example from the dataset\nexample_summary, example_dialogue = train_data.iloc[10]\nprint(f\"Dialogue: \\n{example_dialogue}\")\nprint(f\"\\nSummary: \\n{example_summary}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:21.884768Z","iopub.execute_input":"2024-09-30T19:48:21.885482Z","iopub.status.idle":"2024-09-30T19:48:22.228806Z","shell.execute_reply.started":"2024-09-30T19:48:21.885409Z","shell.execute_reply":"2024-09-30T19:48:22.227343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Dialogue: \nLucas: Hey! How was your day?\nDemi: Hey there! \nDemi: It was pretty fine, actually, thank you!\nDemi: I just got promoted! :D\nLucas: Whoa! Great news!\nLucas: Congratulations!\nLucas: Such a success has to be celebrated.\nDemi: I agree! :D\nDemi: Tonight at Death & Co.?\nLucas: Sure!\nLucas: See you there at 10pm?\nDemi: Yeah! See you there! :D\n\nSummary: \nDemi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"code","source":"document, summary = utils.preprocess(train_data)\ndocument_test, summary_test = utils.preprocess(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:22.230425Z","iopub.execute_input":"2024-09-30T19:48:22.230851Z","iopub.status.idle":"2024-09-30T19:48:23.635152Z","shell.execute_reply.started":"2024-09-30T19:48:22.230809Z","shell.execute_reply":"2024-09-30T19:48:23.628172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(document[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:23.638294Z","iopub.execute_input":"2024-09-30T19:48:23.642390Z","iopub.status.idle":"2024-09-30T19:48:23.661550Z","shell.execute_reply.started":"2024-09-30T19:48:23.642322Z","shell.execute_reply":"2024-09-30T19:48:23.660265Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Concatenating summaries and docs to prepare before passing into the tokenizer","metadata":{}},{"cell_type":"code","source":"docs_and_summary = pd.concat([document, summary], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:23.663272Z","iopub.execute_input":"2024-09-30T19:48:23.663993Z","iopub.status.idle":"2024-09-30T19:48:23.677709Z","shell.execute_reply.started":"2024-09-30T19:48:23.663942Z","shell.execute_reply":"2024-09-30T19:48:23.676268Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Punctuation filtering","metadata":{}},{"cell_type":"code","source":"def apply_filters(text):\n    filters = r'[!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n]'\n    if isinstance(text, str):\n        preprocessed_text = re.sub(filters, ' ', text)\n        \n    else:\n        preprocessed_text = [re.sub(filters, ' ', sentence) for sentence in text]\n    \n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:23.680249Z","iopub.execute_input":"2024-09-30T19:48:23.681026Z","iopub.status.idle":"2024-09-30T19:48:23.692080Z","shell.execute_reply.started":"2024-09-30T19:48:23.680974Z","shell.execute_reply":"2024-09-30T19:48:23.690810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Applying filtering on training data before tokenizing","metadata":{}},{"cell_type":"code","source":"filtered_docs_and_summary = apply_filters(docs_and_summary)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:23.699769Z","iopub.execute_input":"2024-09-30T19:48:23.701336Z","iopub.status.idle":"2024-09-30T19:48:24.312669Z","shell.execute_reply.started":"2024-09-30T19:48:23.701265Z","shell.execute_reply":"2024-09-30T19:48:24.311332Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# PT","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n\ntokenizer = Tokenizer(models.WordLevel(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\ntrainer = trainers.WordLevelTrainer(vocab_size=34249, special_tokens=['[UNK]'])\n\ntokenizer.train_from_iterator(filtered_docs_and_summary, trainer)\n\npytorch_vocab = [token for token in tokenizer.get_vocab().keys()]\nprint(len(pytorch_vocab))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-30T19:48:24.314387Z","iopub.execute_input":"2024-09-30T19:48:24.315135Z","iopub.status.idle":"2024-09-30T19:48:25.619655Z","shell.execute_reply.started":"2024-09-30T19:48:24.315079Z","shell.execute_reply":"2024-09-30T19:48:25.618186Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"34249\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TF","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nfilters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n\noov_token = '[UNK]'\n\ntokenizer = Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n\ntokenizer.fit_on_texts(docs_and_summary.tolist())\n\ntf_vocab = [key for key in tokenizer.word_index.keys()]\nprint(len(tf_vocab))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:25.621767Z","iopub.execute_input":"2024-09-30T19:48:25.622335Z","iopub.status.idle":"2024-09-30T19:48:42.177707Z","shell.execute_reply.started":"2024-09-30T19:48:25.622281Z","shell.execute_reply":"2024-09-30T19:48:42.176375Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"34249\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_pytorch_tokens = list(set(pytorch_vocab) - set(tf_vocab))\nunique_tf_tokens = list(set(tf_vocab) - set(pytorch_vocab))\n\nprint(unique_tf_tokens)\nprint(unique_pytorch_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:48:42.180871Z","iopub.execute_input":"2024-09-30T19:48:42.181555Z","iopub.status.idle":"2024-09-30T19:48:42.210961Z","shell.execute_reply.started":"2024-09-30T19:48:42.181509Z","shell.execute_reply":"2024-09-30T19:48:42.209511Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['\\u3000', '⊂二二二（\\u3000＾ω＾）二⊃', 'comes（\\u3000ﾟдﾟ）', '（\\u3000´']\n['⊂二二二（', 'ﾟдﾟ）', 'comes（', '＾ω＾）二⊃']\n","output_type":"stream"}]}]}
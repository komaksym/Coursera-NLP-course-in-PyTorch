{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9516043,"sourceType":"datasetVersion","datasetId":5791338}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cd /kaggle/input/transformer-summarizer-ds","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:07.012710Z","iopub.execute_input":"2024-10-02T21:23:07.013153Z","iopub.status.idle":"2024-10-02T21:23:07.024533Z","shell.execute_reply.started":"2024-10-02T21:23:07.013095Z","shell.execute_reply":"2024-10-02T21:23:07.022858Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/transformer-summarizer-ds\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport pdb\nimport time\nimport utils\nimport torch\nimport textwrap\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n\n\nwrapper = textwrap.TextWrapper(width=70)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T21:23:07.037584Z","iopub.execute_input":"2024-10-02T21:23:07.038139Z","iopub.status.idle":"2024-10-02T21:23:11.015388Z","shell.execute_reply.started":"2024-10-02T21:23:07.038082Z","shell.execute_reply":"2024-10-02T21:23:11.014036Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/transformer-summarizer-ds/train.json\n/kaggle/input/transformer-summarizer-ds/test.json\n/kaggle/input/transformer-summarizer-ds/utils.py\n/kaggle/input/transformer-summarizer-ds/val.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import the Dataset","metadata":{}},{"cell_type":"code","source":"train_data, test_data = utils.get_train_test_data()\n\n# A single example from the dataset\nexample_summary, example_dialogue = train_data.iloc[10]\nprint(f\"Dialogue: \\n{example_dialogue}\")\nprint(f\"\\nSummary: \\n{example_summary}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:11.020708Z","iopub.execute_input":"2024-10-02T21:23:11.021443Z","iopub.status.idle":"2024-10-02T21:23:11.467363Z","shell.execute_reply.started":"2024-10-02T21:23:11.021383Z","shell.execute_reply":"2024-10-02T21:23:11.463695Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Dialogue: \nLucas: Hey! How was your day?\nDemi: Hey there! \nDemi: It was pretty fine, actually, thank you!\nDemi: I just got promoted! :D\nLucas: Whoa! Great news!\nLucas: Congratulations!\nLucas: Such a success has to be celebrated.\nDemi: I agree! :D\nDemi: Tonight at Death & Co.?\nLucas: Sure!\nLucas: See you there at 10pm?\nDemi: Yeah! See you there! :D\n\nSummary: \nDemi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"code","source":"document, summary = utils.preprocess(train_data)\ndocument_test, summary_test = utils.preprocess(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:11.469393Z","iopub.execute_input":"2024-10-02T21:23:11.469888Z","iopub.status.idle":"2024-10-02T21:23:12.722114Z","shell.execute_reply.started":"2024-10-02T21:23:11.469833Z","shell.execute_reply":"2024-10-02T21:23:12.720895Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(document[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:12.723729Z","iopub.execute_input":"2024-10-02T21:23:12.724098Z","iopub.status.idle":"2024-10-02T21:23:12.729872Z","shell.execute_reply.started":"2024-10-02T21:23:12.724061Z","shell.execute_reply":"2024-10-02T21:23:12.728670Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Concatenating summaries and docs to prepare before passing into the tokenizer","metadata":{}},{"cell_type":"code","source":"docs_and_summary = pd.concat([document, summary], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:12.733195Z","iopub.execute_input":"2024-10-02T21:23:12.733713Z","iopub.status.idle":"2024-10-02T21:23:12.741537Z","shell.execute_reply.started":"2024-10-02T21:23:12.733653Z","shell.execute_reply":"2024-10-02T21:23:12.740286Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Punctuation filtering","metadata":{}},{"cell_type":"code","source":"def apply_filters(text):\n    filters = r'[!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n]'\n    if isinstance(text, str):\n        preprocessed_text = re.sub(filters, ' ', text)\n        \n    else:\n        preprocessed_text = [re.sub(filters, ' ', sentence) for sentence in text]\n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:12.742818Z","iopub.execute_input":"2024-10-02T21:23:12.743176Z","iopub.status.idle":"2024-10-02T21:23:12.752103Z","shell.execute_reply.started":"2024-10-02T21:23:12.743136Z","shell.execute_reply":"2024-10-02T21:23:12.750839Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Applying filtering on training data before tokenizing","metadata":{}},{"cell_type":"code","source":"filtered_docs_and_summary = apply_filters(docs_and_summary)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:12.753488Z","iopub.execute_input":"2024-10-02T21:23:12.754351Z","iopub.status.idle":"2024-10-02T21:23:13.160252Z","shell.execute_reply.started":"2024-10-02T21:23:12.754276Z","shell.execute_reply":"2024-10-02T21:23:13.158820Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"\ntokenizer = Tokenizer(models.WordLevel(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\ntrainer = trainers.WordLevelTrainer(vocab_size=34249, special_tokens=['[PAD]', '[UNK]'])\n\ntokenizer.train_from_iterator(filtered_docs_and_summary, trainer)\n\nvocabulary = tokenizer.get_vocab()\nvocab_size = tokenizer.get_vocab_size() + 1\nprint(f'Size of vocabulary: {vocab_size}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-02T21:23:13.161778Z","iopub.execute_input":"2024-10-02T21:23:13.162165Z","iopub.status.idle":"2024-10-02T21:23:13.926349Z","shell.execute_reply.started":"2024-10-02T21:23:13.162125Z","shell.execute_reply":"2024-10-02T21:23:13.925082Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Size of vocabulary: 34250\n","output_type":"stream"}]},{"cell_type":"code","source":"vocabulary = sorted(vocabulary.items(), key=lambda key: key[1])\nprint(vocabulary[:10])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:13.927749Z","iopub.execute_input":"2024-10-02T21:23:13.928220Z","iopub.status.idle":"2024-10-02T21:23:13.953574Z","shell.execute_reply.started":"2024-10-02T21:23:13.928165Z","shell.execute_reply":"2024-10-02T21:23:13.952500Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[('[PAD]', 0), ('[UNK]', 1), ('i', 2), ('the', 3), ('to', 4), ('you', 5), ('a', 6), ('[EOS]', 7), ('[SOS]', 8), ('and', 9)]\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded = tokenizer.encode(apply_filters(document[0]))\nprint(encoded.ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:13.954995Z","iopub.execute_input":"2024-10-02T21:23:13.955365Z","iopub.status.idle":"2024-10-02T21:23:13.967861Z","shell.execute_reply.started":"2024-10-02T21:23:13.955292Z","shell.execute_reply":"2024-10-02T21:23:13.966718Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[8, 454, 2, 3505, 1613, 30, 5, 81, 50, 619, 66, 454, 63, 220, 5, 98, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(apply_filters(document[0]))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:13.969170Z","iopub.execute_input":"2024-10-02T21:23:13.969539Z","iopub.status.idle":"2024-10-02T21:23:13.979414Z","shell.execute_reply.started":"2024-10-02T21:23:13.969502Z","shell.execute_reply":"2024-10-02T21:23:13.978258Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[SOS] amanda  i baked  cookies  do you want some   jerry  sure   amanda  i'll bring you tomorrow     [EOS]\n","output_type":"stream"}]},{"cell_type":"code","source":"encoder_maxlen = 150\ndecoder_maxlen = 50\n\n# Padding & truncating documents (inputs)\ntokenizer.enable_padding(length=encoder_maxlen)\ntokenizer.enable_truncation(encoder_maxlen)\n\ninputs = tokenizer.encode_batch(apply_filters(document))\n\n# Padding & truncating documents (inputs)\ntokenizer.enable_padding(length=decoder_maxlen)\ntokenizer.enable_truncation(max_length=decoder_maxlen)\n\ntargets = tokenizer.encode_batch(apply_filters(summary))\n\n# Extracting ids only\ninputs = torch.tensor([seq.ids for seq in inputs])\ntargets = torch.tensor([seq.ids for seq in targets])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:13.980704Z","iopub.execute_input":"2024-10-02T21:23:13.981051Z","iopub.status.idle":"2024-10-02T21:23:17.254948Z","shell.execute_reply.started":"2024-10-02T21:23:13.981014Z","shell.execute_reply":"2024-10-02T21:23:17.253751Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, inputs, targets):\n        self.inputs, self.targets = inputs, targets\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.256736Z","iopub.execute_input":"2024-10-02T21:23:17.257135Z","iopub.status.idle":"2024-10-02T21:23:17.264113Z","shell.execute_reply.started":"2024-10-02T21:23:17.257096Z","shell.execute_reply":"2024-10-02T21:23:17.262749Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(inputs, targets)\n\nBATCH_SIZE = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.269583Z","iopub.execute_input":"2024-10-02T21:23:17.270450Z","iopub.status.idle":"2024-10-02T21:23:17.277778Z","shell.execute_reply.started":"2024-10-02T21:23:17.270402Z","shell.execute_reply":"2024-10-02T21:23:17.276379Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"def positional_encoding(num_positions, d_model):\n    position = np.arange(num_positions)[:, np.newaxis]\n    i = k // 2\n    \n    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n    angle_rads = position * angle_rates\n    \n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1:2])    \n    \n    pos_encoding = angle_rads[np.newaxis, ...]\n    return torch.tensor(pos_encoding, torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.279088Z","iopub.execute_input":"2024-10-02T21:23:17.279499Z","iopub.status.idle":"2024-10-02T21:23:17.287546Z","shell.execute_reply.started":"2024-10-02T21:23:17.279450Z","shell.execute_reply":"2024-10-02T21:23:17.286366Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Masking","metadata":{}},{"cell_type":"code","source":"def create_padding_mask(token_ids):\n    seq = torch.logical_not(torch.eq(torch.tensor(token_ids), 0)).float()\n    return torch.unsqueeze(seq, 1)\n\n\ndef create_look_ahead_mask(sequence_length, num_heads):\n    mask = torch.tril(torch.ones((1 * num_heads, sequence_length, sequence_length)))\n    return mask","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.288796Z","iopub.execute_input":"2024-10-02T21:23:17.289162Z","iopub.status.idle":"2024-10-02T21:23:17.302232Z","shell.execute_reply.started":"2024-10-02T21:23:17.289125Z","shell.execute_reply":"2024-10-02T21:23:17.300855Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Self attention","metadata":{}},{"cell_type":"markdown","source":"## Scaled dot product attention","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = torch.matmul(q, k.T)\n\n    dk = torch.tensor(k.size(-1), dtype=torch.float)\n    scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n    scaled_attention_logits = torch.unsqueeze(scaled_attention_logits, dim=0)\n    \n    if mask is not None:\n        scaled_attention_logits += (1. - mask) * -1e9\n        \n    attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n    \n    output = torch.matmul(attention_weights, v)\n    return output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.303853Z","iopub.execute_input":"2024-10-02T21:23:17.304234Z","iopub.status.idle":"2024-10-02T21:23:17.313433Z","shell.execute_reply.started":"2024-10-02T21:23:17.304196Z","shell.execute_reply":"2024-10-02T21:23:17.312291Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Test your function!\nq = torch.tensor([[1, 1, 0, 1], [0, 1, 1, 1], [1, 0, 1, 1]]).float()\nk = torch.tensor([[1, 1, 0, 1], [1, 0, 1, 1 ], [1, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]).float()\nv = torch.tensor([[0, 0], [1, 0], [1, 0], [1, 1], [1, 1]]).float()\nmask = torch.tensor([[[0, 1, 0, 1, 1], [1, 0, 0, 1, 1], [1, 1, 0, 1, 1]]])\n\nou, atw = scaled_dot_product_attention(q, k, v, mask)\nou = torch.round(ou, decimals=2)\natw = torch.round(atw, decimals=2)\n\nprint(f\"Output:\\n {ou}\")\nprint(f\"\\nAttention weigths:\\n {atw}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.314896Z","iopub.execute_input":"2024-10-02T21:23:17.315348Z","iopub.status.idle":"2024-10-02T21:23:17.443297Z","shell.execute_reply.started":"2024-10-02T21:23:17.315278Z","shell.execute_reply":"2024-10-02T21:23:17.442146Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Output:\n tensor([[[1.0000, 0.6200],\n         [0.6200, 0.6200],\n         [0.7400, 0.3100]]])\n\nAttention weigths:\n tensor([[[0.0000, 0.3800, 0.0000, 0.2300, 0.3800],\n         [0.3800, 0.0000, 0.0000, 0.2300, 0.3800],\n         [0.2600, 0.4300, 0.0000, 0.1600, 0.1600]]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"def FullyConnected(embedding_dim, fully_connected_dim):\n    return nn.Sequential(\n        nn.Linear(embedding_dim, fully_connected_dim),\n        nn.ReLU(),\n        nn.Linear(fully_connected_dim, embedding_dim)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.444651Z","iopub.execute_input":"2024-10-02T21:23:17.445004Z","iopub.status.idle":"2024-10-02T21:23:17.451443Z","shell.execute_reply.started":"2024-10-02T21:23:17.444967Z","shell.execute_reply":"2024-10-02T21:23:17.449937Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Encoder Layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n        super().__init__()\n        \n        self.mha = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_rate, batch_first=True)\n        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n        \n        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n        \n        self.dropout_ffn = nn.Dropout(dropout_rate)\n    \n    def forward(self, x, mask):\n        mha_output, _ = self.mha(x, x, x, mask)\n        skip_x_attention = self.layernorm1(x + mha_output)\n        \n        ffn_output = self.ffn(skip_x_attention)\n        ffn_output = self.dropout_ffn(ffn_output)\n        encoder_layer_out = self.layernorm2(ffn_output + skip_x_attention)\n        \n        return encoder_layer_out","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.453062Z","iopub.execute_input":"2024-10-02T21:23:17.453791Z","iopub.status.idle":"2024-10-02T21:23:17.464769Z","shell.execute_reply.started":"2024-10-02T21:23:17.453733Z","shell.execute_reply":"2024-10-02T21:23:17.463683Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Full Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n                maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super().__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n        \n        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim, \n                                        num_heads=num_heads,\n                                        fully_connected_dim=fully_connected_dim,\n                                        dropout_rate=dropout_rate,\n                                        layernorm_eps=layernorm_eps) \n                           for _ in range(num_layers)]\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x, mask):\n        seq_len = x.size(1)\n\n        x = self.embedding(x)\n        x *= torch.sqrt(self.embedding_dim).float()\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, mask)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:23:17.466056Z","iopub.execute_input":"2024-10-02T21:23:17.466465Z","iopub.status.idle":"2024-10-02T21:23:17.479111Z","shell.execute_reply.started":"2024-10-02T21:23:17.466426Z","shell.execute_reply":"2024-10-02T21:23:17.478052Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Decoder","metadata":{}},{"cell_type":"markdown","source":"## Decoder Layer","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm=1e-6):\n        super().__init__()\n        \n        self.mha1 = nn.MultiheadAttention(embedding_dim, num_heads, dropout_rate, batch_first=True)\n        self.mha2 = nn.MultiheadAttention(embedding_dim, num_heads, dropout_rate, batch_first=True)\n        \n        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n        \n        self.layernorm1 = nn.LayerNorm(embedding_dim, layernorm)\n        self.layernorm2 = nn.LayerNorm(embedding_dim, layernorm)\n        self.layernorm3 = nn.LayerNorm(embedding_dim, layernorm)\n        \n        self.dropout_ffn = nn.Dropout(dropout_rate)\n        \n    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n        \n        mult_attn_out1, attn_weights_block1 = self.mha1(x, x, x, attn_mask=look_ahead_mask, average_attn_weights=False)\n        Q1 = self.layernorm1(x + mult_attn_out1)\n\n        mult_attn_out2, attn_weights_block2 = self.mha2(x, enc_output, enc_output, key_padding_mask=padding_mask, average_attn_weights=False)\n        mult_attn_out2 = self.layernorm2(Q1 + mult_attn_out2)\n\n        ffn_output = self.ffn(mult_attn_out2)\n        ffn_output = self.dropout_ffn(ffn_output)\n        out3 = self.layernorm3(ffn_output + mult_attn_out2)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:36:22.318861Z","iopub.execute_input":"2024-10-02T21:36:22.319420Z","iopub.status.idle":"2024-10-02T21:36:22.332138Z","shell.execute_reply.started":"2024-10-02T21:36:22.319372Z","shell.execute_reply":"2024-10-02T21:36:22.330568Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Full Decoder","metadata":{}},{"cell_type":"code","source":"key_dim = 192\nn_heads = 16\n\ndecoderLayer_test = DecoderLayer(embedding_dim=key_dim, num_heads=n_heads, fully_connected_dim=32)\n\nq = torch.ones((1, 15, key_dim))\nencoder_test_output = torch.tensor(np.random.rand(1, 7, key_dim)).float()\nlook_ahead_mask = create_look_ahead_mask(q.shape[1], n_heads)\n\nout, attn_w_b1, attn_w_b2 = decoderLayer_test(q, encoder_test_output, look_ahead_mask, None)\n\nprint(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\nprint(f\"q has shape:{q.shape}\")\nprint(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n\nprint(f\"Output of decoder layer has shape:{out.shape}\")\nprint(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\nprint(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T21:36:42.040822Z","iopub.execute_input":"2024-10-02T21:36:42.041759Z","iopub.status.idle":"2024-10-02T21:36:42.060150Z","shell.execute_reply.started":"2024-10-02T21:36:42.041710Z","shell.execute_reply":"2024-10-02T21:36:42.058824Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Using embedding_dim=192 and num_heads=16:\n\nq has shape:torch.Size([1, 15, 192])\nOutput of encoder has shape:torch.Size([1, 7, 192])\n\nOutput of decoder layer has shape:torch.Size([1, 15, 192])\nAtt Weights Block 1 has shape:torch.Size([1, 16, 15, 15])\nAtt Weights Block 2 has shape:torch.Size([1, 16, 15, 7])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9600794,"sourceType":"datasetVersion","datasetId":5837173},{"sourceId":4250,"sourceType":"modelInstanceVersion","modelInstanceId":3046,"modelId":575}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"cd '/kaggle/input/c4w3-files'","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:52.764363Z","iopub.execute_input":"2024-10-11T22:21:52.764740Z","iopub.status.idle":"2024-10-11T22:21:52.772145Z","shell.execute_reply.started":"2024-10-11T22:21:52.764704Z","shell.execute_reply":"2024-10-11T22:21:52.771193Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"/kaggle/input/c4w3-files\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport pdb\nimport time\nimport json\nimport torch\nimport string\nimport textwrap\nimport traceback\nimport itertools\nimport numpy as np \nimport torch.nn as nn\nfrom termcolor import colored\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformer_utils_PT\nimport utils_PT\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nwrapper = textwrap.TextWrapper(width=70)\nnp.random.seed(42)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\\","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T22:21:52.795169Z","iopub.execute_input":"2024-10-11T22:21:52.795481Z","iopub.status.idle":"2024-10-11T22:21:52.805729Z","shell.execute_reply.started":"2024-10-11T22:21:52.795450Z","shell.execute_reply":"2024-10-11T22:21:52.804778Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"/kaggle/input/flan-t5/pytorch/small/2/config.json\n/kaggle/input/flan-t5/pytorch/small/2/spiece.model\n/kaggle/input/flan-t5/pytorch/small/2/README.md\n/kaggle/input/flan-t5/pytorch/small/2/tokenizer.json\n/kaggle/input/flan-t5/pytorch/small/2/tf_model.h5\n/kaggle/input/flan-t5/pytorch/small/2/tokenizer_config.json\n/kaggle/input/flan-t5/pytorch/small/2/pytorch_model.bin\n/kaggle/input/flan-t5/pytorch/small/2/special_tokens_map.json\n/kaggle/input/flan-t5/pytorch/small/2/.gitattributes\n/kaggle/input/flan-t5/pytorch/small/2/flax_model.msgpack\n/kaggle/input/flan-t5/pytorch/small/2/generation_config.json\n/kaggle/input/c4w3-files/c4-en-10k.json\n/kaggle/input/c4w3-files/sentencepiece.model\n/kaggle/input/c4w3-files/train-v2.0.json\n/kaggle/input/c4w3-files/transformer_utils_PT.py\n/kaggle/input/c4w3-files/inputs_targets_pairs_file.txt\n/kaggle/input/c4w3-files/utils_PT.py\n/kaggle/input/c4w3-files/c4-en-10k.jsonl\n/kaggle/input/c4w3-files/data.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare the data for pretraining T5","metadata":{}},{"cell_type":"markdown","source":"## C4 dataset","metadata":{}},{"cell_type":"code","source":"# Load example jsons\nwith open('c4-en-10k.jsonl', 'r') as file:\n    example_jsons = [json.loads(line.strip()) for line in file]\n    \n# Printing the examples to see how the data looks like\nfor i in range(5):\n    print(f\"example number {i+1}: \\n\\n{example_jsons[i]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:52.835871Z","iopub.execute_input":"2024-10-11T22:21:52.836126Z","iopub.status.idle":"2024-10-11T22:21:52.996522Z","shell.execute_reply.started":"2024-10-11T22:21:52.836098Z","shell.execute_reply":"2024-10-11T22:21:52.995590Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"example number 1: \n\n{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'}\n\nexample number 2: \n\n{'text': 'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.'}\n\nexample number 3: \n\n{'text': 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.'}\n\nexample number 4: \n\n{'text': \"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\"}\n\nexample number 5: \n\n{'text': 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what’s included in the mill levy measure.'}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Process C4\n","metadata":{}},{"cell_type":"code","source":"# Grab text field from dictionary\nnatural_language_texts = [example_json['text'] for example_json in example_jsons]\n\n# Print the first text example\nprint(natural_language_texts[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:52.998025Z","iopub.execute_input":"2024-10-11T22:21:52.998359Z","iopub.status.idle":"2024-10-11T22:21:53.005189Z","shell.execute_reply.started":"2024-10-11T22:21:52.998298Z","shell.execute_reply":"2024-10-11T22:21:53.004355Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Decode to Natural Language","metadata":{}},{"cell_type":"code","source":"# Uploading the sentencepiece tokenizer\nfrom transformers import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained('/kaggle/input/c4w3-files/sentencepiece.model', clean_up_tokenization_spaces=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.006320Z","iopub.execute_input":"2024-10-11T22:21:53.006689Z","iopub.status.idle":"2024-10-11T22:21:53.302889Z","shell.execute_reply.started":"2024-10-11T22:21:53.006640Z","shell.execute_reply":"2024-10-11T22:21:53.302157Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# ID of the EOS token\neos = tokenizer.convert_tokens_to_ids(\"</s>\")\n\nprint(\"EOS: \" + str(eos))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.304087Z","iopub.execute_input":"2024-10-11T22:21:53.304492Z","iopub.status.idle":"2024-10-11T22:21:53.309708Z","shell.execute_reply.started":"2024-10-11T22:21:53.304448Z","shell.execute_reply":"2024-10-11T22:21:53.308714Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"EOS: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Printing the encoding of each word to see how subwords are tokenized\ntokenized_text = [(list(tokenizer.encode(word, add_special_tokens=False)), word) for word in natural_language_texts[2].split()]\n\nprint(\"Word\\t\\t-->\\tTokenization\")\nprint(\"-\"*40)\nfor element in tokenized_text:\n    print(f\"{element[1]:<8}\\t-->\\t{element[0]}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.313158Z","iopub.execute_input":"2024-10-11T22:21:53.313578Z","iopub.status.idle":"2024-10-11T22:21:53.323781Z","shell.execute_reply.started":"2024-10-11T22:21:53.313535Z","shell.execute_reply":"2024-10-11T22:21:53.322769Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Word\t\t-->\tTokenization\n----------------------------------------\nFoil    \t-->\t[4452, 173]\nplaid   \t-->\t[30772]\nlycra   \t-->\t[3, 120, 2935]\nand     \t-->\t[11]\nspandex \t-->\t[8438, 26, 994]\nshortall\t-->\t[710, 1748]\nwith    \t-->\t[28]\nmetallic\t-->\t[18813]\nslinky  \t-->\t[3, 7, 4907, 63]\ninsets. \t-->\t[16, 2244, 7, 5]\nAttached\t-->\t[28416, 15, 26]\nmetallic\t-->\t[18813]\nelastic \t-->\t[15855]\nbelt    \t-->\t[6782]\nwith    \t-->\t[28]\nO-ring. \t-->\t[411, 18, 1007, 5]\nHeadband\t-->\t[3642, 3348]\nincluded.\t-->\t[1285, 5]\nGreat   \t-->\t[1651]\nhip     \t-->\t[5436]\nhop     \t-->\t[13652]\nor      \t-->\t[42]\njazz    \t-->\t[9948]\ndance   \t-->\t[2595]\ncostume.\t-->\t[11594, 5]\nMade    \t-->\t[6465]\nin      \t-->\t[16]\nthe     \t-->\t[8]\nUSA.    \t-->\t[2312, 5]\n","output_type":"stream"}]},{"cell_type":"code","source":"# We can see that detokenize successfully undoes the tokenization\nprint(f\"tokenized: {tokenizer.encode('Beginners')}\\ndetokenized: {tokenizer.decode(tokenizer.encode('Beginners'), skip_special_tokens=True)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.324824Z","iopub.execute_input":"2024-10-11T22:21:53.325122Z","iopub.status.idle":"2024-10-11T22:21:53.339171Z","shell.execute_reply.started":"2024-10-11T22:21:53.325092Z","shell.execute_reply":"2024-10-11T22:21:53.338238Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"tokenized: [12847, 277, 1]\ndetokenized: Beginners\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_sentinels(tokenizer, display=False):\n    sentinels = {}\n    vocab_size = tokenizer.vocab_size\n    for i, char in enumerate(reversed(string.ascii_letters), 1):\n        decoded_text = tokenizer.decode([vocab_size - i])\n        \n        # Sentinels, ex: <Z> - <a>\n        sentinels[decoded_text] = f\"<{char}>\"\n        \n        if display:\n            print(f\"The sentinel is <{char}> and the decoded token is: {decoded_text}\")\n            \n    return sentinels\n\n\ndef regex_replace(input_str, pattern, replace_with):\n    return re.sub(pattern, replace_with, input_str)\n\n\ndef pretty_decode(encoded_str_list, sentinels, tokenizer):\n    # If already a string, just do the replacements.\n    if isinstance(encoded_str_list, str):\n        for token, char in sentinels.items():\n            encoded_str_list = regex_replace(encoded_str_list, token, char)\n            \n        return encoded_str_list\n    \n    return pretty_decode(tokenizer.decode(encoded_str_list, skip_special_tokens=True), sentinels, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.340171Z","iopub.execute_input":"2024-10-11T22:21:53.340463Z","iopub.status.idle":"2024-10-11T22:21:53.351051Z","shell.execute_reply.started":"2024-10-11T22:21:53.340421Z","shell.execute_reply":"2024-10-11T22:21:53.350209Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"sentinels = get_sentinels(tokenizer, display=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.352068Z","iopub.execute_input":"2024-10-11T22:21:53.352401Z","iopub.status.idle":"2024-10-11T22:21:53.376541Z","shell.execute_reply.started":"2024-10-11T22:21:53.352370Z","shell.execute_reply":"2024-10-11T22:21:53.375691Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"The sentinel is <Z> and the decoded token is: Internațional\nThe sentinel is <Y> and the decoded token is: erwachsene\nThe sentinel is <X> and the decoded token is: Cushion\nThe sentinel is <W> and the decoded token is: imunitar\nThe sentinel is <V> and the decoded token is: Intellectual\nThe sentinel is <U> and the decoded token is: traditi\nThe sentinel is <T> and the decoded token is: disguise\nThe sentinel is <S> and the decoded token is: exerce\nThe sentinel is <R> and the decoded token is: nourishe\nThe sentinel is <Q> and the decoded token is: predominant\nThe sentinel is <P> and the decoded token is: amitié\nThe sentinel is <O> and the decoded token is: erkennt\nThe sentinel is <N> and the decoded token is: dimension\nThe sentinel is <M> and the decoded token is: inférieur\nThe sentinel is <L> and the decoded token is: refugi\nThe sentinel is <K> and the decoded token is: cheddar\nThe sentinel is <J> and the decoded token is: unterlieg\nThe sentinel is <I> and the decoded token is: garanteaz\nThe sentinel is <H> and the decoded token is: făcute\nThe sentinel is <G> and the decoded token is: réglage\nThe sentinel is <F> and the decoded token is: pedepse\nThe sentinel is <E> and the decoded token is: Germain\nThe sentinel is <D> and the decoded token is: distinctly\nThe sentinel is <C> and the decoded token is: Schraub\nThe sentinel is <B> and the decoded token is: emanat\nThe sentinel is <A> and the decoded token is: trimestre\nThe sentinel is <z> and the decoded token is: disrespect\nThe sentinel is <y> and the decoded token is: Erasmus\nThe sentinel is <x> and the decoded token is: Australia\nThe sentinel is <w> and the decoded token is: permeabil\nThe sentinel is <v> and the decoded token is: deseori\nThe sentinel is <u> and the decoded token is: manipulated\nThe sentinel is <t> and the decoded token is: suggér\nThe sentinel is <s> and the decoded token is: corespund\nThe sentinel is <r> and the decoded token is: nitro\nThe sentinel is <q> and the decoded token is: oyons\nThe sentinel is <p> and the decoded token is: Account\nThe sentinel is <o> and the decoded token is: échéan\nThe sentinel is <n> and the decoded token is: laundering\nThe sentinel is <m> and the decoded token is: genealogy\nThe sentinel is <l> and the decoded token is: QuickBooks\nThe sentinel is <k> and the decoded token is: constituted\nThe sentinel is <j> and the decoded token is: Fertigung\nThe sentinel is <i> and the decoded token is: goutte\nThe sentinel is <h> and the decoded token is: regulă\nThe sentinel is <g> and the decoded token is: overwhelmingly\nThe sentinel is <f> and the decoded token is: émerg\nThe sentinel is <e> and the decoded token is: broyeur\nThe sentinel is <d> and the decoded token is: povești\nThe sentinel is <c> and the decoded token is: emulator\nThe sentinel is <b> and the decoded token is: halloween\nThe sentinel is <a> and the decoded token is: combustibil\n","output_type":"stream"}]},{"cell_type":"code","source":"pretty_decode('I want to dress up as an Intellectual this halloween.', sentinels, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.377519Z","iopub.execute_input":"2024-10-11T22:21:53.377795Z","iopub.status.idle":"2024-10-11T22:21:53.388448Z","shell.execute_reply.started":"2024-10-11T22:21:53.377767Z","shell.execute_reply":"2024-10-11T22:21:53.387726Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"'I want to dress up as an <V> this <b>.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing and masking","metadata":{}},{"cell_type":"code","source":"def tokenize_and_mask(text, noise=0.15, randomizer=np.random.uniform, tokenizer=None):\n    # Current sentinel number (starts at 0)\n    cur_sentinel_num = 0\n    \n    # Inputs and targets\n    inps, targs = [], []\n\n    # Vocab_size\n    vocab_size = tokenizer.vocab_size\n    \n    # EOS token id \n    # Must be at the end of each target!\n    eos = tokenizer.convert_tokens_to_ids('</s>')\n    \n    # prev_no_mask is True if the previous token was NOT masked, False otherwise\n    # set prev_no_mask to True\n    prev_no_mask = True\n    \n    # Loop over the tokenized text\n    for token in tokenizer.encode(text, add_special_tokens=False):\n        \n        # Generate a random value between 0 and 1\n        rnd_val = randomizer()\n        \n        # Check if the noise is greater than a random value (weighted coin flip)\n        if noise > rnd_val:\n            \n            # Check if previous token was NOT masked\n            if prev_no_mask:\n                \n                # Current sentinel increases by 1\n                cur_sentinel_num += 1\n                \n                # Compute end_id by subtracting current sentinel value out of the total vocabulary size\n                end_id = vocab_size - cur_sentinel_num\n                \n                # Append end_id at the end of the targets\n                targs.append(end_id)\n                \n                # Append end_id at the end of the inputs\n                inps.append(end_id)\n            \n            # Append token at the end of the targets\n            targs.append(token)\n            \n            # set prev_no_mask accordingly\n            prev_no_mask = False\n            \n        else:\n            # Append token at the end of the inputs\n            inps.append(token)\n            \n            # Set prev_no_mask accordingly\n            prev_no_mask = True\n            \n    targs.append(eos)\n    return inps, targs","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.390828Z","iopub.execute_input":"2024-10-11T22:21:53.391154Z","iopub.status.idle":"2024-10-11T22:21:53.399313Z","shell.execute_reply.started":"2024-10-11T22:21:53.391124Z","shell.execute_reply":"2024-10-11T22:21:53.398513Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Some logic to mock a np.random value generator\n# Needs to be in the same cell for it to always generate same outpu\ndef testing_rnd():\n    def dummy_generator():\n        vals = np.linspace(0, 1, 10)\n        cyclic_vals = itertools.cycle(vals)\n        \n        for _ in range(100):\n            yield next(cyclic_vals)\n            \n    dumr = itertools.cycle(dummy_generator())\n    \n    def dummy_randomizer():\n        return next(dumr)\n    \n    return dummy_randomizer\n\n\ninput_str = 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers.'\ninps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd(), tokenizer=tokenizer)\nprint(f\"tokenized inputs - shape={len(inps)}:\\n\\n{inps}\\n\\ntargets - shape={len(targs)}:\\n\\n{targs}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.400503Z","iopub.execute_input":"2024-10-11T22:21:53.400821Z","iopub.status.idle":"2024-10-11T22:21:53.412979Z","shell.execute_reply.started":"2024-10-11T22:21:53.400791Z","shell.execute_reply":"2024-10-11T22:21:53.412109Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"tokenized inputs - shape=53:\n\n[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5]\n\ntargets - shape=19:\n\n[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Inputs: \\n\\n', pretty_decode(inps, sentinels, tokenizer))\nprint('\\nTargets: \\n\\n', pretty_decode(targs, sentinels, tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.414095Z","iopub.execute_input":"2024-10-11T22:21:53.414580Z","iopub.status.idle":"2024-10-11T22:21:53.442637Z","shell.execute_reply.started":"2024-10-11T22:21:53.414538Z","shell.execute_reply":"2024-10-11T22:21:53.441835Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Inputs: \n\n <Z> BBQ Class Taking Place in Missoul <Y> Do you want to get better at making <X>? You will have the opportunity, put <W> your calendar now. Thursday, September 22 <V> World Class BBQ Champion, Tony Balay <U>onestar Smoke Rangers.\n\nTargets: \n\n <Z> Beginners <Y>a! <X> delicious BBQ <W> this on <V>nd join <U> from L\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating the pairs ","metadata":{}},{"cell_type":"code","source":"# Apply tokenize_and_mask\ninputs_targets_pairs = [tokenize_and_mask(text.encode('utf-8', errors='ignore').decode('utf-8'), tokenizer=tokenizer) \n                        for text in natural_language_texts[:2000]]","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:21:53.443724Z","iopub.execute_input":"2024-10-11T22:21:53.444006Z","iopub.status.idle":"2024-10-11T22:22:03.646637Z","shell.execute_reply.started":"2024-10-11T22:21:53.443965Z","shell.execute_reply":"2024-10-11T22:22:03.645862Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def display_input_target_pairs(inputs_targets_pairs, sentinels, \n                               wrapper=textwrap.TextWrapper(width=70), tokenizer=tokenizer):\n    for i, inp_tar_pair in enumerate(inputs_targets_pairs, 1):\n        inps, tgts = inp_tar_pair\n        inps = pretty_decode(inps, sentinels, tokenizer)\n        tgts = pretty_decode(tgts, sentinels, tokenizer)\n        \n        print(f\"[{i}]\\n\\n\"\n              f\"inputs:\\n{wrapper.fill(inps)}\\n\\n\"\n              f\"targets:\\n{wrapper.fill(tgts)}\\n\\n\\n\")\n\n# Print 3 samples. We print inputs with less than 100 tokens. It is just to give you and idea of the process        \ndisplay_input_target_pairs(filter(lambda x: len(x[0]) < 100, inputs_targets_pairs[:12]), sentinels)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:03.650265Z","iopub.execute_input":"2024-10-11T22:22:03.650566Z","iopub.status.idle":"2024-10-11T22:22:03.714179Z","shell.execute_reply.started":"2024-10-11T22:22:03.650535Z","shell.execute_reply":"2024-10-11T22:22:03.713369Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"[1]\n\ninputs:\n<Z>il plaid <Y>lycra <X> spandex shortall with metallic slinky\n<W>sets. Attache <V> metallic elastic belt with O <U>ring. Head <T>\nincluded. Great hip hop<S> jazz dance costume.<R> in the USA.\n\ntargets:\n<Z> Fo <Y>  <X> and <W> in <V>d <U>- <T>band<S> or<R> Made\n\n\n\n[2]\n\ninputs:\nI thought I was going to <Z> 3rd season <Y> Wire tonight. <X> there\nwas a commentary <W> 11, so I had to re <V>watch <U> Ground with <T>\ncommentary. Hopefully<S> can finish<R> season <Q>.\n\ntargets:\n<Z> finish the <Y> of the <X> But <W> on episode <V>- <U> Middle <T>\nthe<S> I<R> the <Q> next weekend\n\n\n\n[3]\n\ninputs:\nPencarian <Z>FILM Untuk \" <Y>eace <X>er 2017 <W> yuk mampir ke channel\nsay <V>. Edges <U> provides the l.. A corrupt cop makes one w.. <T>er\n2017 <S>.. Náo Lon - Peace Break.. Please subscribe and hit..<R> in HD\nat http://.. <Q> cannot believe I manage..\n\ntargets:\n<Z>  <Y>P <X> Break <W>\" <V>. <U> East <T> Peace Break<S> <R> uploaded\n<Q> I\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pretrain a T5 model using C4","metadata":{}},{"cell_type":"markdown","source":"##  Instantiate a new transformer model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:03.715189Z","iopub.execute_input":"2024-10-11T22:22:03.715484Z","iopub.status.idle":"2024-10-11T22:22:03.743734Z","shell.execute_reply.started":"2024-10-11T22:22:03.715452Z","shell.execute_reply":"2024-10-11T22:22:03.742805Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Define the model parameters\nnum_layers = 2\nembedding_dim = 128\nfully_connected_dim = 128\nnum_heads = 2\npositional_encoding_length = 256\n\nencoder_vocab_size = tokenizer.vocab_size\ndecoder_vocab_size = encoder_vocab_size\n\n# Initialize the model\ntransformer = transformer_utils_PT.Transformer(\n    num_layers, \n    embedding_dim, \n    num_heads, \n    fully_connected_dim,\n    encoder_vocab_size, \n    decoder_vocab_size, \n    positional_encoding_length, \n    positional_encoding_length,\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:03.744902Z","iopub.execute_input":"2024-10-11T22:22:03.745285Z","iopub.status.idle":"2024-10-11T22:22:04.128377Z","shell.execute_reply.started":"2024-10-11T22:22:03.745253Z","shell.execute_reply":"2024-10-11T22:22:04.127574Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"#learning_rate = transformer_utils_PT.CustomSchedule(embedding_dim)\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\ncriterion = nn.CrossEntropyLoss(reduction='none')","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:04.129377Z","iopub.execute_input":"2024-10-11T22:22:04.129667Z","iopub.status.idle":"2024-10-11T22:22:04.911110Z","shell.execute_reply.started":"2024-10-11T22:22:04.129635Z","shell.execute_reply":"2024-10-11T22:22:04.910355Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"## C4 pretraining","metadata":{}},{"cell_type":"code","source":"# Limit the size of the input and output data so this can run in this environment\nencoder_maxlen = 150\ndecoder_maxlen = 50\n\ninputs = tokenizer.pad([{\"input_ids\": x[0]} for x in inputs_targets_pairs], max_length=encoder_maxlen, padding='max_length')\ninputs = torch.tensor([seq[:encoder_maxlen] for seq in inputs['input_ids']])\ntargets = tokenizer.pad([{\"input_ids\": x[1]} for x in inputs_targets_pairs], max_length=decoder_maxlen, padding='max_length')\ntargets = torch.tensor([seq[:decoder_maxlen] for seq in targets['input_ids']])","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:04.912190Z","iopub.execute_input":"2024-10-11T22:22:04.912701Z","iopub.status.idle":"2024-10-11T22:22:05.132423Z","shell.execute_reply.started":"2024-10-11T22:22:04.912668Z","shell.execute_reply":"2024-10-11T22:22:05.131509Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, inputs, targets):\n        self.inputs, self.targets = inputs, targets\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return (self.inputs[idx], self.targets[idx])\n    \n\nBATCH_SIZE = 64\n\ntrain_dataset = CustomDataset(inputs, targets)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:05.133609Z","iopub.execute_input":"2024-10-11T22:22:05.133973Z","iopub.status.idle":"2024-10-11T22:22:05.140616Z","shell.execute_reply.started":"2024-10-11T22:22:05.133928Z","shell.execute_reply":"2024-10-11T22:22:05.139703Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nnum_of_batches = len(train_loader)\nepoch_losses = []\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    transformer.train()\n    \n    for i, (inputs, targets) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        tar_inp = targets[:, :-1].to(device)\n        tar_real = targets[:, 1:].to(device)\n        \n        # Masks\n        enc_padding_mask = transformer_utils_PT.create_padding_mask(inputs).to(device)\n        look_ahead_mask = transformer_utils_PT.create_look_ahead_mask(tar_inp.shape[1], len(inputs) * num_heads).to(device)\n        dec_padding_mask = transformer_utils_PT.create_padding_mask(inputs).to(device)\n        \n        # Your common forw/back prop loop\n        optimizer.zero_grad()\n        predictions, _ = transformer(\n            inputs, tar_inp, True, enc_padding_mask,\n            look_ahead_mask, dec_padding_mask\n        )\n        \n        loss = transformer_utils_PT.masked_loss(predictions.to(device), tar_real)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    # Calculating epoch losses    \n    epoch_losses.append(running_loss / num_of_batches)\n        \n    print(f'Epoch {epoch+1}, Loss {epoch_losses[-1]}')","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:05.141832Z","iopub.execute_input":"2024-10-11T22:22:05.142213Z","iopub.status.idle":"2024-10-11T22:22:31.747665Z","shell.execute_reply.started":"2024-10-11T22:22:05.142171Z","shell.execute_reply":"2024-10-11T22:22:31.746709Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Epoch 1, Loss 10.492356657981873\nEpoch 2, Loss 10.357855945825577\nEpoch 3, Loss 10.228851228952408\nEpoch 4, Loss 10.104453593492508\nEpoch 5, Loss 9.976301193237305\nEpoch 6, Loss 9.853137820959091\nEpoch 7, Loss 9.721908271312714\nEpoch 8, Loss 9.594953566789627\nEpoch 9, Loss 9.468185484409332\nEpoch 10, Loss 9.342415690422058\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Uploading a pretrained model","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/small/2\", clean_up_tokenization_spaces=True)\nmodel = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/flan-t5/pytorch/small/2\", device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:31.748933Z","iopub.execute_input":"2024-10-11T22:22:31.749244Z","iopub.status.idle":"2024-10-11T22:22:38.101883Z","shell.execute_reply.started":"2024-10-11T22:22:31.749211Z","shell.execute_reply":"2024-10-11T22:22:38.101104Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# Fine tune the T5 model for Question Answering","metadata":{}},{"cell_type":"code","source":"with open('train-v2.0.json', 'r') as f:\n    example_jsons = json.load(f)\n    \n\nexample_jsons = example_jsons['data']\n\nprint('Number of articles: ' + str(len(example_jsons)))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:38.102969Z","iopub.execute_input":"2024-10-11T22:22:38.103268Z","iopub.status.idle":"2024-10-11T22:22:39.489559Z","shell.execute_reply.started":"2024-10-11T22:22:38.103236Z","shell.execute_reply":"2024-10-11T22:22:39.488625Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Number of articles: 442\n","output_type":"stream"}]},{"cell_type":"code","source":"example_article = example_jsons[0]\n\nprint(\"Title: \" + example_article['title'])\nprint(example_article['paragraphs'][0])","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:39.490713Z","iopub.execute_input":"2024-10-11T22:22:39.491013Z","iopub.status.idle":"2024-10-11T22:22:39.496083Z","shell.execute_reply.started":"2024-10-11T22:22:39.490981Z","shell.execute_reply":"2024-10-11T22:22:39.495133Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Title: Beyoncé\n{'qas': [{'question': 'When did Beyonce start becoming popular?', 'id': '56be85543aeaaa14008c9063', 'answers': [{'text': 'in the late 1990s', 'answer_start': 269}], 'is_impossible': False}, {'question': 'What areas did Beyonce compete in when she was growing up?', 'id': '56be85543aeaaa14008c9065', 'answers': [{'text': 'singing and dancing', 'answer_start': 207}], 'is_impossible': False}, {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\", 'id': '56be85543aeaaa14008c9066', 'answers': [{'text': '2003', 'answer_start': 526}], 'is_impossible': False}, {'question': 'In what city and state did Beyonce  grow up? ', 'id': '56bf6b0f3aeaaa14008c9601', 'answers': [{'text': 'Houston, Texas', 'answer_start': 166}], 'is_impossible': False}, {'question': 'In which decade did Beyonce become famous?', 'id': '56bf6b0f3aeaaa14008c9602', 'answers': [{'text': 'late 1990s', 'answer_start': 276}], 'is_impossible': False}, {'question': 'In what R&B group was she the lead singer?', 'id': '56bf6b0f3aeaaa14008c9603', 'answers': [{'text': \"Destiny's Child\", 'answer_start': 320}], 'is_impossible': False}, {'question': 'What album made her a worldwide known artist?', 'id': '56bf6b0f3aeaaa14008c9604', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}, {'question': \"Who managed the Destiny's Child group?\", 'id': '56bf6b0f3aeaaa14008c9605', 'answers': [{'text': 'Mathew Knowles', 'answer_start': 360}], 'is_impossible': False}, {'question': 'When did Beyoncé rise to fame?', 'id': '56d43c5f2ccc5a1400d830a9', 'answers': [{'text': 'late 1990s', 'answer_start': 276}], 'is_impossible': False}, {'question': \"What role did Beyoncé have in Destiny's Child?\", 'id': '56d43c5f2ccc5a1400d830aa', 'answers': [{'text': 'lead singer', 'answer_start': 290}], 'is_impossible': False}, {'question': 'What was the first album Beyoncé released as a solo artist?', 'id': '56d43c5f2ccc5a1400d830ab', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}, {'question': 'When did Beyoncé release Dangerously in Love?', 'id': '56d43c5f2ccc5a1400d830ac', 'answers': [{'text': '2003', 'answer_start': 526}], 'is_impossible': False}, {'question': 'How many Grammy awards did Beyoncé win for her first solo album?', 'id': '56d43c5f2ccc5a1400d830ad', 'answers': [{'text': 'five', 'answer_start': 590}], 'is_impossible': False}, {'question': \"What was Beyoncé's role in Destiny's Child?\", 'id': '56d43ce42ccc5a1400d830b4', 'answers': [{'text': 'lead singer', 'answer_start': 290}], 'is_impossible': False}, {'question': \"What was the name of Beyoncé's first solo album?\", 'id': '56d43ce42ccc5a1400d830b5', 'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}], 'is_impossible': False}], 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating a list of paired question and answers\n## Parse the SQuaD 2.0 Dataset","metadata":{}},{"cell_type":"code","source":"def parse_squad(dataset):\n    \n    inputs, targets = [], []\n    \n    # Loop over all the articles\n    for article in dataset:\n        \n        # Loop over each paragraph of each article\n        for paragraph in article['paragraphs']:\n            \n            # Extract context from the paragraph\n            context = paragraph['context']\n            \n            # Loop over each question of the given paragraph\n            for qa in paragraph['qas']:\n                \n                # If this question is not impossible and there is at least one answer\n                if(len(qa['answers']) > 0 and not (qa['is_impossible'])):\n                    \n                    # Create the question/context sequence\n                    question_context = 'question: ' + qa['question'] + ' context: ' + context\n                    \n                    # Create the answer sequence. Use the text field of the first answer\n                    answer = 'answer: ' + qa['answers'][0]['text']\n                    \n                    # Add the question_context to the inputs list\n                    inputs.append(question_context)\n                    \n                    # Add the answer to the targets list\n                    targets.append(answer)\n                    \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:39.497601Z","iopub.execute_input":"2024-10-11T22:22:39.498404Z","iopub.status.idle":"2024-10-11T22:22:39.507802Z","shell.execute_reply.started":"2024-10-11T22:22:39.498361Z","shell.execute_reply":"2024-10-11T22:22:39.507022Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"inputs, targets = parse_squad(example_jsons)\nprint(\"Number of question/answer pairs: \" + str(len(inputs)))\n\nprint('\\nFirst Q/A pair:\\n\\ninputs: ' + colored(inputs[0], 'blue'))\nprint('\\ntargets: ' + colored(targets[0], 'green'))\nprint('\\nLast Q/A pair:\\n\\ninputs: ' + colored(inputs[-1], 'blue'))\nprint('\\ntargets: ' + colored(targets[-1], 'green'))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:39.508776Z","iopub.execute_input":"2024-10-11T22:22:39.509140Z","iopub.status.idle":"2024-10-11T22:22:39.695339Z","shell.execute_reply.started":"2024-10-11T22:22:39.509109Z","shell.execute_reply":"2024-10-11T22:22:39.694436Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Number of question/answer pairs: 86821\n\nFirst Q/A pair:\n\ninputs: \u001b[34mquestion: When did Beyonce start becoming popular? context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\u001b[0m\n\ntargets: \u001b[32manswer: in the late 1990s\u001b[0m\n\nLast Q/A pair:\n\ninputs: \u001b[34mquestion: What is KMC an initialism of? context: Kathmandu Metropolitan City (KMC), in order to promote international relations has established an International Relations Secretariat (IRC). KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States. This activity has been further enhanced by establishing formal relationships with 8 other cities: Motsumoto City of Japan, Rochester of the USA, Yangon (formerly Rangoon) of Myanmar, Xi'an of the People's Republic of China, Minsk of Belarus, and Pyongyang of the Democratic Republic of Korea. KMC's constant endeavor is to enhance its interaction with SAARC countries, other International agencies and many other major cities of the world to achieve better urban management and developmental programs for Kathmandu.\u001b[0m\n\ntargets: \u001b[32manswer: Kathmandu Metropolitan City\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# 40K pairs for training\ninputs_train = inputs[:40000]\ntargets_train = targets[:40000]\n\n# 5K pairs for testing\ninputs_test = inputs[40000:]\ntargets_test = targets[40000:]","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:39.696565Z","iopub.execute_input":"2024-10-11T22:22:39.696871Z","iopub.status.idle":"2024-10-11T22:22:39.704295Z","shell.execute_reply.started":"2024-10-11T22:22:39.696839Z","shell.execute_reply":"2024-10-11T22:22:39.703513Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# Limit the size of the input and output data so this can run in this environment\nencoder_maxlen = 150\ndecoder_maxlen = 50\n\ninputs = tokenizer(inputs_train, max_length=encoder_maxlen, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=False).input_ids\ntargets = tokenizer(targets_train, max_length=decoder_maxlen, truncation=True, padding='max_length', return_tensors='pt').input_ids\n\nBATCH_SIZE = 64\n\ntrain_dataset = CustomDataset(inputs, targets)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:22:39.705897Z","iopub.execute_input":"2024-10-11T22:22:39.706342Z","iopub.status.idle":"2024-10-11T22:23:33.148479Z","shell.execute_reply.started":"2024-10-11T22:22:39.706286Z","shell.execute_reply":"2024-10-11T22:23:33.147608Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"## Fine tune the T5 model","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:23:33.149675Z","iopub.execute_input":"2024-10-11T22:23:33.150040Z","iopub.status.idle":"2024-10-11T22:23:33.157208Z","shell.execute_reply.started":"2024-10-11T22:23:33.149998Z","shell.execute_reply":"2024-10-11T22:23:33.156364Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"\"\"\"epochs = 10\n\nnum_of_batches = len(train_loader)\nepoch_losses = []\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    model.train()\n    \n    for i, (inputs, targets) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        tar_inp = targets[:, :-1].to(device)\n        tar_real = targets[:, 1:].to(device)\n        \n        #Masks\n        enc_padding_mask = transformer_utils_PT.create_padding_mask(inputs).to(device)\n        look_ahead_mask = transformer_utils_PT.create_look_ahead_mask(tar_inp.shape[1], len(inputs) * num_heads).to(device)\n        dec_padding_mask = transformer_utils_PT.create_padding_mask(tar_inp).to(device=device, dtype=torch.bool)\n        \n        optimizer.zero_grad()\n        predictions = model(\n            input_ids=inputs, attention_mask=enc_padding_mask, decoder_input_ids=tar_inp,\n            decoder_attention_mask=dec_padding_mask, return_dict=False\n        )\n        #pdb.set_trace()\n        predictions = torch.transpose(predictions[0], 1, 2)\n        loss = transformer_utils_PT.masked_loss(predictions.to(device), tar_real)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    epoch_losses.append(running_loss / num_of_batches)\n        \n    print(f'Epoch {epoch+1}, Loss {epoch_losses[-1]}')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:23:33.158352Z","iopub.execute_input":"2024-10-11T22:23:33.158628Z","iopub.status.idle":"2024-10-11T22:23:33.173185Z","shell.execute_reply.started":"2024-10-11T22:23:33.158598Z","shell.execute_reply":"2024-10-11T22:23:33.172285Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"\"epochs = 10\\n\\nnum_of_batches = len(train_loader)\\nepoch_losses = []\\n\\nfor epoch in range(epochs):\\n    running_loss = 0.0\\n    model.train()\\n    \\n    for i, (inputs, targets) in enumerate(train_loader):\\n        inputs = inputs.to(device)\\n        tar_inp = targets[:, :-1].to(device)\\n        tar_real = targets[:, 1:].to(device)\\n        \\n        #Masks\\n        enc_padding_mask = transformer_utils_PT.create_padding_mask(inputs).to(device)\\n        look_ahead_mask = transformer_utils_PT.create_look_ahead_mask(tar_inp.shape[1], len(inputs) * num_heads).to(device)\\n        dec_padding_mask = transformer_utils_PT.create_padding_mask(tar_inp).to(device=device, dtype=torch.bool)\\n        \\n        optimizer.zero_grad()\\n        predictions = model(\\n            input_ids=inputs, attention_mask=enc_padding_mask, decoder_input_ids=tar_inp,\\n            decoder_attention_mask=dec_padding_mask, return_dict=False\\n        )\\n        #pdb.set_trace()\\n        predictions = torch.transpose(predictions[0], 1, 2)\\n        loss = transformer_utils_PT.masked_loss(predictions.to(device), tar_real)\\n        loss.backward()\\n        optimizer.step()\\n        \\n        running_loss += loss.item()\\n        \\n    epoch_losses.append(running_loss / num_of_batches)\\n        \\n    print(f'Epoch {epoch+1}, Loss {epoch_losses[-1]}')\""},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading finetuned pretrained flan-T5-small","metadata":{}},{"cell_type":"code","source":"#model.save_pretrained('/kaggle/working/flan-T5_finetuned', from_pt=True)\nmodel = T5ForConditionalGeneration.from_pretrained('/kaggle/working/flan-T5_small_finetuned').to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:23:33.174483Z","iopub.execute_input":"2024-10-11T22:23:33.174742Z","iopub.status.idle":"2024-10-11T22:23:33.738441Z","shell.execute_reply.started":"2024-10-11T22:23:33.174713Z","shell.execute_reply":"2024-10-11T22:23:33.737514Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"## Implement your own Question Answering model","metadata":{}},{"cell_type":"code","source":"# Define an example question\nexample_question = \"question: What color is the sky? context: Sky is blue\"\n\n# Question is tokenized and padded\n# Note that this is hardcoded here but you must implement this in the upcoming exercise\ntokenized_padded_question = torch.tensor([[822, 10, 363, 945, 19, 8, 5796, 58, 2625, 10, 5643, 19, 1692, 0, 0]]).to(device)\n\n# All answers begin with the string \"answer: \"\n# Feel free to check that this is indeed the tokenized version of that string\ntokenized_answer = torch.tensor([[1525, 10]]).to(device)\n\n# Predict the next word using the transformer_utils.next_word function\n# Notice that it expects the question, answer and model (in that order)\nnext_word = transformer_utils_PT.next_word(model, tokenized_padded_question, tokenized_answer)\nprint(f\"Predicted next word is : {tokenizer.batch_decode(next_word)}\")\n\n# Concatenate predicted word with answer so far\nanswer_so_far = torch.cat([tokenized_answer, next_word], dim=-1)\n\nprint(f\"Answer so far: {tokenizer.batch_decode(answer_so_far)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T22:24:55.530768Z","iopub.execute_input":"2024-10-11T22:24:55.531700Z","iopub.status.idle":"2024-10-11T22:24:55.571695Z","shell.execute_reply.started":"2024-10-11T22:24:55.531659Z","shell.execute_reply":"2024-10-11T22:24:55.570714Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Predicted next word is : ['blue']\nAnswer so far: ['answer: blue']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Implement the question answering function","metadata":{}},{"cell_type":"code","source":"def answer_question(question, model, tokenizer, encoder_maxlen=150, decoder_maxlen=50):\n    # Tokenize, pad, truncate the question\n    tokenized_question = (tokenizer(question, max_length=encoder_maxlen, truncation=True, padding='max_length', return_tensors='pt', add_special_tokens=False).input_ids).to(device)\n    \n    # Tokenize, pad, truncate the answer\n    tokenized_answer = (tokenizer('answer: ', max_length=decoder_maxlen, truncation=True, padding='max_length', return_tensors='pt').input_ids).to(device)\n    eos = tokenizer.convert_tokens_to_ids(\"</s>\") \n    \n    # Generate the answers until the max seq length is reached or the EOS token\n    for i in range(decoder_maxlen):\n        next_word = transformer_utils_PT.next_word(model, tokenized_question, tokenized_answer).to(device)\n        \n        tokenized_answer = torch.cat([tokenized_answer, next_word], axis=1)\n        \n        if next_word == eos:\n            break\n\n    return tokenized_answer","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:28:16.175416Z","iopub.execute_input":"2024-10-11T10:28:16.175896Z","iopub.status.idle":"2024-10-11T10:28:16.183344Z","shell.execute_reply.started":"2024-10-11T10:28:16.175854Z","shell.execute_reply":"2024-10-11T10:28:16.182379Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"idx = 10408\n\nresult = answer_question(inputs_train[idx], model, tokenizer)\nresult = torch.squeeze(result)\nprint(colored(pretty_decode(result, sentinels, tokenizer), 'blue'))\nprint()\nprint(inputs_train[idx])\nprint(colored(targets_train[idx], 'green'))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:28:16.313351Z","iopub.execute_input":"2024-10-11T10:28:16.313883Z","iopub.status.idle":"2024-10-11T10:28:16.441462Z","shell.execute_reply.started":"2024-10-11T10:28:16.313850Z","shell.execute_reply":"2024-10-11T10:28:16.440579Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\u001b[34manswer: January 9, 1957\u001b[0m\n\nquestion: When was the Chechen-Ingush Autonomous Soviet Socialist Republic transferred from the Georgian SSR? context: On January 9, 1957, Karachay Autonomous Oblast and Chechen-Ingush Autonomous Soviet Socialist Republic were restored by Khrushchev and they were transferred from the Georgian SSR back to the Russian SFSR.\n\u001b[32manswer: January 9, 1957\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 311\nresult = answer_question(inputs_test[idx], model, tokenizer)\nresult = torch.squeeze(result)\nprint(colored(pretty_decode(result, sentinels, tokenizer), 'blue'))\nprint()\nprint(inputs_test[idx])\nprint(colored(targets_test[idx], 'green'))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:28:37.251885Z","iopub.execute_input":"2024-10-11T10:28:37.252765Z","iopub.status.idle":"2024-10-11T10:28:37.326399Z","shell.execute_reply.started":"2024-10-11T10:28:37.252724Z","shell.execute_reply":"2024-10-11T10:28:37.325515Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"\u001b[34manswer: 15\u001b[0m\n\nquestion:  On what date was a state funeral held for Napoleon? context: In 1840, Louis Philippe I obtained permission from the British to return Napoleon's remains to France. On 15 December 1840, a state funeral was held. The hearse proceeded from the Arc de Triomphe down the Champs-Élysées, across the Place de la Concorde to the Esplanade des Invalides and then to the cupola in St Jérôme's Chapel, where it remained until the tomb designed by Louis Visconti was completed. In 1861, Napoleon's remains were entombed in a porphyry sarcophagus in the crypt under the dome at Les Invalides.\n\u001b[32manswer: 15 December 1840\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 1231\nresult = answer_question(inputs_test[idx], model, tokenizer)\nresult = torch.squeeze(result)\nprint(colored(pretty_decode(result, sentinels, tokenizer), 'blue'))\nprint()\nprint(inputs_test[idx])\nprint(colored(targets_test[idx], 'green'))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:29:13.003440Z","iopub.execute_input":"2024-10-11T10:29:13.004075Z","iopub.status.idle":"2024-10-11T10:29:13.077057Z","shell.execute_reply.started":"2024-10-11T10:29:13.004038Z","shell.execute_reply":"2024-10-11T10:29:13.076090Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"\u001b[34manswer: 2001\u001b[0m\n\nquestion: When did the WIPO adopt 'The Geneva Declaration on the Future of the World Intellectual Property Organization'? context: The World Intellectual Property Organization (WIPO) recognizes that conflicts may exist between the respect for and implementation of current intellectual property systems and other human rights. In 2001 the UN Committee on Economic, Social and Cultural Rights issued a document called \"Human rights and intellectual property\" that argued that intellectual property tends to be governed by economic goals when it should be viewed primarily as a social product; in order to serve human well-being, intellectual property systems must respect and conform to human rights laws. According to the Committee, when systems fail to do so they risk infringing upon the human right to food and health, and to cultural participation and scientific benefits. In 2004 the General Assembly of WIPO adopted The Geneva Declaration on the Future of the World Intellectual Property Organization which argues that WIPO should \"focus more on the needs of developing countries, and to view IP as one of many tools for development—not as an end in itself\".\n\u001b[32manswer: 2004\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 32\nresult = answer_question(inputs_test[idx], model, tokenizer)\nresult = torch.squeeze(result)\nprint(colored(pretty_decode(result, sentinels, tokenizer), 'blue'))\nprint()\nprint(inputs_test[idx])\nprint(colored(targets_test[idx], 'green'))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T10:30:03.360454Z","iopub.execute_input":"2024-10-11T10:30:03.360842Z","iopub.status.idle":"2024-10-11T10:30:03.437837Z","shell.execute_reply.started":"2024-10-11T10:30:03.360806Z","shell.execute_reply":"2024-10-11T10:30:03.436931Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"\u001b[34manswer: Russia\u001b[0m\n\nquestion: What nation did Napoleon invade in 1812 to enforce his blockade? context: Tensions over rising Polish nationalism and the economic effects of the Continental System led to renewed confrontation with Russia. To enforce his blockade, Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic collapse of the Grand Army, forcing the French to retreat, as well as leading to the widespread destruction of Russian lands and cities. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France. A chaotic military campaign in Central Europe eventually culminated in a large Allied army defeating Napoleon at the Battle of Leipzig in October. The next year, the Allies invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba. The Bourbons were restored to power and the French lost most of the territories that they had conquered since the Revolution. However, Napoleon escaped from Elba in February 1815 and took control of the government once again. The Allies responded by forming a Seventh Coalition, which ultimately defeated Napoleon at the Battle of Waterloo in June. The Royal Navy then thwarted his planned escape to the United States in July, so he surrendered to the British after running out of other options. The British exiled him to the remote island of Saint Helena in the South Atlantic. His death in 1821 at the age of 51 was received with shock and grief throughout Europe. In 1840, a million people witnessed his remains returning to Paris, where they still reside at Les Invalides.\n\u001b[32manswer: Russia\u001b[0m\n","output_type":"stream"}]}]}